{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Begin Training\n",
      "Epoch:  1, learning rate = 0.1\n",
      "[2022-05-10 05:40:19]    Batch:   10, Loss:5.065450, AvgAcc:10.6250%\n",
      "[2022-05-10 05:40:28]    Batch:   20, Loss:2.432364, AvgAcc:11.4062%\n",
      "[2022-05-10 05:40:36]    Batch:   30, Loss:2.360449, AvgAcc:11.9531%\n",
      "[2022-05-10 05:40:45]    Batch:   40, Loss:2.323648, AvgAcc:11.6602%\n",
      "[2022-05-10 05:40:53]    Batch:   50, Loss:2.263470, AvgAcc:11.5469%\n",
      "[2022-05-10 05:41:01]    Batch:   60, Loss:2.504783, AvgAcc:11.4844%\n",
      "[2022-05-10 05:41:10]    Batch:   70, Loss:2.281576, AvgAcc:11.3393%\n",
      "[2022-05-10 05:41:18]    Batch:   80, Loss:2.285307, AvgAcc:11.2598%\n",
      "[2022-05-10 05:41:26]    Batch:   90, Loss:2.296317, AvgAcc:11.3281%\n",
      "[2022-05-10 05:41:35]    Batch:  100, Loss:2.399196, AvgAcc:11.4922%\n",
      "[2022-05-10 05:41:43]    Batch:  110, Loss:2.295704, AvgAcc:11.5838%\n",
      "[2022-05-10 05:41:52]    Batch:  120, Loss:2.267356, AvgAcc:11.9076%\n",
      "[2022-05-10 05:42:00]    Batch:  130, Loss:2.301147, AvgAcc:12.0072%\n",
      "[2022-05-10 05:42:09]    Batch:  140, Loss:2.285812, AvgAcc:11.9922%\n",
      "[2022-05-10 05:42:17]    Batch:  150, Loss:2.281054, AvgAcc:12.1042%\n",
      "[2022-05-10 05:42:25]    Batch:  160, Loss:2.280379, AvgAcc:12.2998%\n",
      "[2022-05-10 05:42:34]    Batch:  170, Loss:2.245925, AvgAcc:12.4219%\n",
      "[2022-05-10 05:42:42]    Batch:  180, Loss:2.237593, AvgAcc:12.5477%\n",
      "[2022-05-10 05:42:50]    Batch:  190, Loss:2.237681, AvgAcc:12.6316%\n",
      "[2022-05-10 05:42:58]    Batch:  200, Loss:2.251950, AvgAcc:12.7656%\n",
      "[2022-05-10 05:43:06]    Batch:  210, Loss:2.200638, AvgAcc:12.8906%\n",
      "[2022-05-10 05:43:15]    Batch:  220, Loss:2.148467, AvgAcc:13.0788%\n",
      "[2022-05-10 05:43:23]    Batch:  230, Loss:2.189120, AvgAcc:13.3730%\n",
      "[2022-05-10 05:43:31]    Batch:  240, Loss:2.138715, AvgAcc:13.5710%\n",
      "[2022-05-10 05:43:40]    Batch:  250, Loss:2.147293, AvgAcc:13.8406%\n",
      "[2022-05-10 05:43:48]    Batch:  260, Loss:2.068507, AvgAcc:14.0655%\n",
      "[2022-05-10 05:43:56]    Batch:  270, Loss:2.239679, AvgAcc:14.2188%\n",
      "[2022-05-10 05:44:05]    Batch:  280, Loss:2.153945, AvgAcc:14.4001%\n",
      "[2022-05-10 05:44:13]    Batch:  290, Loss:2.135660, AvgAcc:14.5420%\n",
      "[2022-05-10 05:44:21]    Batch:  300, Loss:2.170545, AvgAcc:14.6797%\n",
      "[2022-05-10 05:44:30]    Batch:  310, Loss:2.071551, AvgAcc:14.9017%\n",
      "[2022-05-10 05:44:38]    Batch:  320, Loss:2.181769, AvgAcc:15.1074%\n",
      "[2022-05-10 05:44:46]    Batch:  330, Loss:2.141290, AvgAcc:15.2580%\n",
      "[2022-05-10 05:44:54]    Batch:  340, Loss:2.063261, AvgAcc:15.4297%\n",
      "[2022-05-10 05:45:03]    Batch:  350, Loss:2.075228, AvgAcc:15.6094%\n",
      "[2022-05-10 05:45:11]    Batch:  360, Loss:2.027749, AvgAcc:15.8485%\n",
      "[2022-05-10 05:45:20]    Batch:  370, Loss:1.974931, AvgAcc:16.0705%\n",
      "[2022-05-10 05:45:31]    Batch:  380, Loss:1.999403, AvgAcc:16.2500%\n",
      "[2022-05-10 05:45:42]    Batch:  390, Loss:1.966240, AvgAcc:16.4744%\n",
      "Epoch:  2, learning rate = 0.1\n",
      "[2022-05-10 05:46:15]    Batch:   10, Loss:2.018334, AvgAcc:24.2969%\n",
      "[2022-05-10 05:46:24]    Batch:   20, Loss:2.033804, AvgAcc:23.5156%\n",
      "[2022-05-10 05:46:32]    Batch:   30, Loss:1.880743, AvgAcc:23.3854%\n",
      "[2022-05-10 05:46:41]    Batch:   40, Loss:2.053762, AvgAcc:24.1211%\n",
      "[2022-05-10 05:46:51]    Batch:   50, Loss:1.927845, AvgAcc:24.8750%\n",
      "[2022-05-10 05:47:29]    Batch:   60, Loss:2.084274, AvgAcc:24.7266%\n",
      "[2022-05-10 05:48:10]    Batch:   70, Loss:1.957209, AvgAcc:24.8214%\n",
      "[2022-05-10 05:49:04]    Batch:   80, Loss:2.078949, AvgAcc:25.1855%\n",
      "[2022-05-10 05:49:52]    Batch:   90, Loss:1.975197, AvgAcc:25.5382%\n",
      "[2022-05-10 05:50:32]    Batch:  100, Loss:2.027251, AvgAcc:25.8516%\n",
      "[2022-05-10 05:51:14]    Batch:  110, Loss:1.963642, AvgAcc:25.9730%\n",
      "[2022-05-10 05:52:07]    Batch:  120, Loss:1.803492, AvgAcc:26.0482%\n",
      "[2022-05-10 05:52:57]    Batch:  130, Loss:2.017959, AvgAcc:26.3642%\n",
      "[2022-05-10 05:53:47]    Batch:  140, Loss:1.900880, AvgAcc:26.4453%\n",
      "[2022-05-10 05:54:41]    Batch:  150, Loss:1.779313, AvgAcc:26.6510%\n",
      "[2022-05-10 05:55:42]    Batch:  160, Loss:1.819808, AvgAcc:26.9092%\n",
      "[2022-05-10 05:56:42]    Batch:  170, Loss:1.714809, AvgAcc:27.2564%\n",
      "[2022-05-10 05:57:25]    Batch:  180, Loss:1.807154, AvgAcc:27.4870%\n",
      "[2022-05-10 05:58:09]    Batch:  190, Loss:1.864780, AvgAcc:27.6562%\n",
      "[2022-05-10 05:58:51]    Batch:  200, Loss:1.879110, AvgAcc:27.8320%\n",
      "[2022-05-10 05:59:35]    Batch:  210, Loss:1.886311, AvgAcc:27.9725%\n",
      "[2022-05-10 06:00:20]    Batch:  220, Loss:1.783946, AvgAcc:28.1250%\n",
      "[2022-05-10 06:01:00]    Batch:  230, Loss:1.761354, AvgAcc:28.3764%\n",
      "[2022-05-10 06:01:40]    Batch:  240, Loss:1.875291, AvgAcc:28.6230%\n",
      "[2022-05-10 06:02:21]    Batch:  250, Loss:1.771694, AvgAcc:28.8438%\n",
      "[2022-05-10 06:03:06]    Batch:  260, Loss:1.634385, AvgAcc:28.9483%\n",
      "[2022-05-10 06:03:51]    Batch:  270, Loss:1.880223, AvgAcc:29.1551%\n",
      "[2022-05-10 06:04:32]    Batch:  280, Loss:1.868362, AvgAcc:29.2941%\n",
      "[2022-05-10 06:05:13]    Batch:  290, Loss:1.748483, AvgAcc:29.3966%\n",
      "[2022-05-10 06:05:54]    Batch:  300, Loss:1.877312, AvgAcc:29.5469%\n",
      "[2022-05-10 06:06:39]    Batch:  310, Loss:1.867721, AvgAcc:29.7455%\n",
      "[2022-05-10 06:07:20]    Batch:  320, Loss:1.966479, AvgAcc:29.8560%\n",
      "[2022-05-10 06:08:01]    Batch:  330, Loss:1.742664, AvgAcc:30.0355%\n",
      "[2022-05-10 06:08:41]    Batch:  340, Loss:1.830818, AvgAcc:30.1677%\n",
      "[2022-05-10 06:09:26]    Batch:  350, Loss:1.845946, AvgAcc:30.2656%\n",
      "[2022-05-10 06:10:07]    Batch:  360, Loss:1.706607, AvgAcc:30.3841%\n",
      "[2022-05-10 06:10:46]    Batch:  370, Loss:1.529623, AvgAcc:30.5089%\n",
      "[2022-05-10 06:11:26]    Batch:  380, Loss:1.723378, AvgAcc:30.5243%\n",
      "[2022-05-10 06:12:06]    Batch:  390, Loss:1.594681, AvgAcc:30.6891%\n",
      "Epoch:  3, learning rate = 0.1\n",
      "[2022-05-10 06:14:21]    Batch:   10, Loss:1.736713, AvgAcc:33.0469%\n",
      "[2022-05-10 06:15:02]    Batch:   20, Loss:1.760182, AvgAcc:33.3594%\n",
      "[2022-05-10 06:15:46]    Batch:   30, Loss:1.602505, AvgAcc:34.4271%\n",
      "[2022-05-10 06:16:29]    Batch:   40, Loss:1.868110, AvgAcc:34.8242%\n",
      "[2022-05-10 06:17:07]    Batch:   50, Loss:1.575232, AvgAcc:35.2188%\n",
      "[2022-05-10 06:17:47]    Batch:   60, Loss:1.769504, AvgAcc:35.3906%\n",
      "[2022-05-10 06:18:29]    Batch:   70, Loss:1.684594, AvgAcc:35.5357%\n",
      "[2022-05-10 06:19:11]    Batch:   80, Loss:1.847102, AvgAcc:35.5859%\n",
      "[2022-05-10 06:19:53]    Batch:   90, Loss:1.813488, AvgAcc:35.6684%\n",
      "[2022-05-10 06:20:34]    Batch:  100, Loss:1.860906, AvgAcc:35.9453%\n",
      "[2022-05-10 06:21:13]    Batch:  110, Loss:1.763464, AvgAcc:36.1648%\n",
      "[2022-05-10 06:21:56]    Batch:  120, Loss:1.616943, AvgAcc:36.1133%\n",
      "[2022-05-10 06:22:39]    Batch:  130, Loss:1.676110, AvgAcc:36.3702%\n",
      "[2022-05-10 06:23:23]    Batch:  140, Loss:1.589217, AvgAcc:36.6183%\n",
      "[2022-05-10 06:24:06]    Batch:  150, Loss:1.587539, AvgAcc:36.8802%\n",
      "[2022-05-10 06:24:48]    Batch:  160, Loss:1.671136, AvgAcc:36.9775%\n",
      "[2022-05-10 06:25:27]    Batch:  170, Loss:1.528008, AvgAcc:37.2105%\n",
      "[2022-05-10 06:26:10]    Batch:  180, Loss:1.596271, AvgAcc:37.3741%\n",
      "[2022-05-10 06:26:50]    Batch:  190, Loss:1.488062, AvgAcc:37.6686%\n",
      "[2022-05-10 06:27:30]    Batch:  200, Loss:1.557099, AvgAcc:37.8516%\n",
      "[2022-05-10 06:28:08]    Batch:  210, Loss:1.585248, AvgAcc:38.0060%\n",
      "[2022-05-10 06:28:50]    Batch:  220, Loss:1.460891, AvgAcc:38.1783%\n",
      "[2022-05-10 06:29:33]    Batch:  230, Loss:1.389098, AvgAcc:38.4511%\n",
      "[2022-05-10 06:30:17]    Batch:  240, Loss:1.703769, AvgAcc:38.6491%\n",
      "[2022-05-10 06:31:02]    Batch:  250, Loss:1.494060, AvgAcc:38.8937%\n",
      "[2022-05-10 06:31:41]    Batch:  260, Loss:1.280503, AvgAcc:39.0655%\n",
      "[2022-05-10 06:32:20]    Batch:  270, Loss:1.612877, AvgAcc:39.1898%\n",
      "[2022-05-10 06:32:59]    Batch:  280, Loss:1.629396, AvgAcc:39.3527%\n",
      "[2022-05-10 06:33:36]    Batch:  290, Loss:1.481119, AvgAcc:39.4855%\n",
      "[2022-05-10 06:34:17]    Batch:  300, Loss:1.527109, AvgAcc:39.5443%\n",
      "[2022-05-10 06:34:59]    Batch:  310, Loss:1.613934, AvgAcc:39.7329%\n",
      "[2022-05-10 06:35:42]    Batch:  320, Loss:1.607581, AvgAcc:39.9414%\n",
      "[2022-05-10 06:36:25]    Batch:  330, Loss:1.434154, AvgAcc:40.1420%\n",
      "[2022-05-10 06:37:06]    Batch:  340, Loss:1.630703, AvgAcc:40.3424%\n",
      "[2022-05-10 06:37:47]    Batch:  350, Loss:1.440322, AvgAcc:40.5357%\n",
      "[2022-05-10 06:38:29]    Batch:  360, Loss:1.510034, AvgAcc:40.6858%\n",
      "[2022-05-10 06:39:12]    Batch:  370, Loss:1.280344, AvgAcc:40.7876%\n",
      "[2022-05-10 06:39:55]    Batch:  380, Loss:1.373474, AvgAcc:40.8779%\n",
      "[2022-05-10 06:40:35]    Batch:  390, Loss:1.391872, AvgAcc:41.0897%\n",
      "Epoch:  4, learning rate = 0.1\n",
      "[2022-05-10 06:42:46]    Batch:   10, Loss:1.335205, AvgAcc:47.4219%\n",
      "[2022-05-10 06:43:29]    Batch:   20, Loss:1.394928, AvgAcc:45.6641%\n",
      "[2022-05-10 06:44:09]    Batch:   30, Loss:1.308382, AvgAcc:46.5885%\n",
      "[2022-05-10 06:44:50]    Batch:   40, Loss:1.638211, AvgAcc:46.6602%\n",
      "[2022-05-10 06:45:31]    Batch:   50, Loss:1.310007, AvgAcc:46.5312%\n",
      "[2022-05-10 06:46:12]    Batch:   60, Loss:1.571988, AvgAcc:46.6146%\n",
      "[2022-05-10 06:46:50]    Batch:   70, Loss:1.387877, AvgAcc:46.5960%\n",
      "[2022-05-10 06:47:30]    Batch:   80, Loss:1.564623, AvgAcc:46.8359%\n",
      "[2022-05-10 06:48:14]    Batch:   90, Loss:1.450758, AvgAcc:46.7795%\n",
      "[2022-05-10 06:48:53]    Batch:  100, Loss:1.575697, AvgAcc:46.8828%\n",
      "[2022-05-10 06:49:33]    Batch:  110, Loss:1.408070, AvgAcc:47.1804%\n",
      "[2022-05-10 06:50:14]    Batch:  120, Loss:1.275819, AvgAcc:47.3893%\n",
      "[2022-05-10 06:50:55]    Batch:  130, Loss:1.380948, AvgAcc:47.6442%\n",
      "[2022-05-10 06:51:38]    Batch:  140, Loss:1.264102, AvgAcc:47.9297%\n",
      "[2022-05-10 06:52:19]    Batch:  150, Loss:1.369018, AvgAcc:48.2396%\n",
      "[2022-05-10 06:52:59]    Batch:  160, Loss:1.492093, AvgAcc:48.3496%\n",
      "[2022-05-10 06:53:39]    Batch:  170, Loss:1.233529, AvgAcc:48.5524%\n",
      "[2022-05-10 06:54:22]    Batch:  180, Loss:1.287349, AvgAcc:48.7066%\n",
      "[2022-05-10 06:55:05]    Batch:  190, Loss:1.275162, AvgAcc:48.9391%\n",
      "[2022-05-10 06:55:46]    Batch:  200, Loss:1.218754, AvgAcc:49.2305%\n",
      "[2022-05-10 06:56:26]    Batch:  210, Loss:1.268232, AvgAcc:49.4829%\n",
      "[2022-05-10 06:57:04]    Batch:  220, Loss:1.209256, AvgAcc:49.5206%\n",
      "[2022-05-10 06:57:45]    Batch:  230, Loss:1.116122, AvgAcc:49.6705%\n",
      "[2022-05-10 06:58:26]    Batch:  240, Loss:1.374832, AvgAcc:49.8210%\n",
      "[2022-05-10 06:59:08]    Batch:  250, Loss:1.385234, AvgAcc:49.9781%\n",
      "[2022-05-10 06:59:53]    Batch:  260, Loss:1.099619, AvgAcc:50.0391%\n",
      "[2022-05-10 07:00:39]    Batch:  270, Loss:1.396430, AvgAcc:50.0174%\n",
      "[2022-05-10 07:01:21]    Batch:  280, Loss:1.374704, AvgAcc:50.0670%\n",
      "[2022-05-10 07:02:01]    Batch:  290, Loss:1.203682, AvgAcc:50.1940%\n",
      "[2022-05-10 07:02:42]    Batch:  300, Loss:1.388911, AvgAcc:50.2839%\n",
      "[2022-05-10 07:03:25]    Batch:  310, Loss:1.426011, AvgAcc:50.4209%\n",
      "[2022-05-10 07:04:07]    Batch:  320, Loss:1.346138, AvgAcc:50.5884%\n",
      "[2022-05-10 07:04:46]    Batch:  330, Loss:1.242938, AvgAcc:50.7150%\n",
      "[2022-05-10 07:05:26]    Batch:  340, Loss:1.321749, AvgAcc:50.8617%\n",
      "[2022-05-10 07:06:06]    Batch:  350, Loss:1.209612, AvgAcc:50.9710%\n",
      "[2022-05-10 07:06:48]    Batch:  360, Loss:1.234287, AvgAcc:51.1003%\n",
      "[2022-05-10 07:07:30]    Batch:  370, Loss:1.153400, AvgAcc:51.2035%\n",
      "[2022-05-10 07:08:13]    Batch:  380, Loss:1.116380, AvgAcc:51.3158%\n",
      "[2022-05-10 07:08:56]    Batch:  390, Loss:1.141781, AvgAcc:51.4623%\n",
      "Epoch:  5, learning rate = 0.1\n",
      "[2022-05-10 07:11:13]    Batch:   10, Loss:1.142089, AvgAcc:57.9688%\n",
      "[2022-05-10 07:11:54]    Batch:   20, Loss:1.151275, AvgAcc:57.8125%\n",
      "[2022-05-10 07:12:34]    Batch:   30, Loss:1.174259, AvgAcc:57.4740%\n",
      "[2022-05-10 07:13:16]    Batch:   40, Loss:1.289529, AvgAcc:57.1484%\n",
      "[2022-05-10 07:13:57]    Batch:   50, Loss:1.113079, AvgAcc:56.6094%\n",
      "[2022-05-10 07:14:39]    Batch:   60, Loss:1.378674, AvgAcc:56.3932%\n",
      "[2022-05-10 07:15:19]    Batch:   70, Loss:1.195829, AvgAcc:56.3170%\n",
      "[2022-05-10 07:16:01]    Batch:   80, Loss:1.249113, AvgAcc:56.6016%\n",
      "[2022-05-10 07:16:40]    Batch:   90, Loss:1.331407, AvgAcc:56.5017%\n",
      "[2022-05-10 07:17:18]    Batch:  100, Loss:1.255238, AvgAcc:56.5859%\n",
      "[2022-05-10 07:18:02]    Batch:  110, Loss:1.210628, AvgAcc:56.6122%\n",
      "[2022-05-10 07:18:45]    Batch:  120, Loss:1.100850, AvgAcc:56.6146%\n",
      "[2022-05-10 07:19:27]    Batch:  130, Loss:1.139135, AvgAcc:56.7909%\n",
      "[2022-05-10 07:20:06]    Batch:  140, Loss:1.065935, AvgAcc:56.9922%\n",
      "[2022-05-10 07:20:47]    Batch:  150, Loss:1.064243, AvgAcc:57.0417%\n",
      "[2022-05-10 07:21:27]    Batch:  160, Loss:1.139566, AvgAcc:57.1094%\n",
      "[2022-05-10 07:22:10]    Batch:  170, Loss:1.076605, AvgAcc:57.3483%\n",
      "[2022-05-10 07:22:52]    Batch:  180, Loss:1.122515, AvgAcc:57.4523%\n",
      "[2022-05-10 07:23:35]    Batch:  190, Loss:1.111416, AvgAcc:57.5493%\n",
      "[2022-05-10 07:24:15]    Batch:  200, Loss:1.036080, AvgAcc:57.6875%\n",
      "[2022-05-10 07:24:56]    Batch:  210, Loss:1.146322, AvgAcc:57.7567%\n",
      "[2022-05-10 07:25:36]    Batch:  220, Loss:1.066081, AvgAcc:57.8729%\n",
      "[2022-05-10 07:26:17]    Batch:  230, Loss:0.818123, AvgAcc:58.0808%\n",
      "[2022-05-10 07:26:56]    Batch:  240, Loss:1.154373, AvgAcc:58.2129%\n",
      "[2022-05-10 07:27:36]    Batch:  250, Loss:1.145599, AvgAcc:58.2906%\n",
      "[2022-05-10 07:28:19]    Batch:  260, Loss:0.934661, AvgAcc:58.3804%\n",
      "[2022-05-10 07:29:02]    Batch:  270, Loss:1.167579, AvgAcc:58.3160%\n",
      "[2022-05-10 07:29:41]    Batch:  280, Loss:1.176778, AvgAcc:58.3119%\n",
      "[2022-05-10 07:30:25]    Batch:  290, Loss:1.065624, AvgAcc:58.3998%\n",
      "[2022-05-10 07:31:08]    Batch:  300, Loss:1.107968, AvgAcc:58.4714%\n",
      "[2022-05-10 07:31:47]    Batch:  310, Loss:1.228091, AvgAcc:58.5559%\n",
      "[2022-05-10 07:32:26]    Batch:  320, Loss:1.113669, AvgAcc:58.6768%\n",
      "[2022-05-10 07:33:11]    Batch:  330, Loss:1.042653, AvgAcc:58.8116%\n",
      "[2022-05-10 07:33:58]    Batch:  340, Loss:1.051487, AvgAcc:58.9085%\n",
      "[2022-05-10 07:34:41]    Batch:  350, Loss:1.025952, AvgAcc:58.9732%\n",
      "[2022-05-10 07:35:21]    Batch:  360, Loss:1.018003, AvgAcc:59.1189%\n",
      "[2022-05-10 07:36:01]    Batch:  370, Loss:1.078347, AvgAcc:59.1596%\n",
      "[2022-05-10 07:36:43]    Batch:  380, Loss:0.876716, AvgAcc:59.2002%\n",
      "[2022-05-10 07:37:25]    Batch:  390, Loss:0.932320, AvgAcc:59.3329%\n",
      "Epoch:  6, learning rate = 0.1\n",
      "[2022-05-10 07:39:49]    Batch:   10, Loss:0.955949, AvgAcc:64.0625%\n",
      "[2022-05-10 07:40:32]    Batch:   20, Loss:0.996578, AvgAcc:63.4766%\n",
      "[2022-05-10 07:41:16]    Batch:   30, Loss:0.919129, AvgAcc:63.5938%\n",
      "[2022-05-10 07:41:56]    Batch:   40, Loss:1.169195, AvgAcc:63.5352%\n",
      "[2022-05-10 07:42:35]    Batch:   50, Loss:0.959391, AvgAcc:62.9844%\n",
      "[2022-05-10 07:43:15]    Batch:   60, Loss:1.182706, AvgAcc:63.1380%\n",
      "[2022-05-10 07:43:56]    Batch:   70, Loss:1.012612, AvgAcc:62.8125%\n",
      "[2022-05-10 07:44:37]    Batch:   80, Loss:1.112594, AvgAcc:62.9492%\n",
      "[2022-05-10 07:45:18]    Batch:   90, Loss:1.114988, AvgAcc:62.8038%\n",
      "[2022-05-10 07:45:58]    Batch:  100, Loss:1.021312, AvgAcc:62.9688%\n",
      "[2022-05-10 07:46:36]    Batch:  110, Loss:1.074564, AvgAcc:63.2741%\n",
      "[2022-05-10 07:47:17]    Batch:  120, Loss:0.928312, AvgAcc:63.3919%\n",
      "[2022-05-10 07:48:01]    Batch:  130, Loss:1.078219, AvgAcc:63.5337%\n",
      "[2022-05-10 07:48:44]    Batch:  140, Loss:0.919074, AvgAcc:63.6830%\n",
      "[2022-05-10 07:49:25]    Batch:  150, Loss:0.947187, AvgAcc:63.6354%\n",
      "[2022-05-10 07:50:08]    Batch:  160, Loss:1.008963, AvgAcc:63.6670%\n",
      "[2022-05-10 07:50:50]    Batch:  170, Loss:1.016616, AvgAcc:63.7546%\n",
      "[2022-05-10 07:51:34]    Batch:  180, Loss:0.964012, AvgAcc:63.8238%\n",
      "[2022-05-10 07:52:16]    Batch:  190, Loss:0.939969, AvgAcc:63.9309%\n",
      "[2022-05-10 07:53:00]    Batch:  200, Loss:0.951882, AvgAcc:63.9062%\n",
      "[2022-05-10 07:53:40]    Batch:  210, Loss:1.063902, AvgAcc:63.9211%\n",
      "[2022-05-10 07:54:21]    Batch:  220, Loss:0.936903, AvgAcc:64.0021%\n",
      "[2022-05-10 07:54:57]    Batch:  230, Loss:0.643411, AvgAcc:64.1678%\n",
      "[2022-05-10 07:55:36]    Batch:  240, Loss:1.053427, AvgAcc:64.2904%\n",
      "[2022-05-10 07:56:16]    Batch:  250, Loss:1.029512, AvgAcc:64.3875%\n",
      "[2022-05-10 07:56:54]    Batch:  260, Loss:0.807907, AvgAcc:64.4531%\n",
      "[2022-05-10 07:57:36]    Batch:  270, Loss:1.080295, AvgAcc:64.4444%\n",
      "[2022-05-10 07:58:18]    Batch:  280, Loss:0.976561, AvgAcc:64.4420%\n",
      "[2022-05-10 07:58:56]    Batch:  290, Loss:0.939959, AvgAcc:64.5016%\n",
      "[2022-05-10 07:59:34]    Batch:  300, Loss:1.094292, AvgAcc:64.5000%\n",
      "[2022-05-10 08:00:12]    Batch:  310, Loss:0.977971, AvgAcc:64.5791%\n",
      "[2022-05-10 08:01:01]    Batch:  320, Loss:1.055152, AvgAcc:64.6289%\n",
      "[2022-05-10 08:01:49]    Batch:  330, Loss:0.853813, AvgAcc:64.7277%\n",
      "[2022-05-10 08:02:43]    Batch:  340, Loss:0.930865, AvgAcc:64.7955%\n",
      "[2022-05-10 08:03:45]    Batch:  350, Loss:0.912617, AvgAcc:64.8214%\n",
      "[2022-05-10 08:04:39]    Batch:  360, Loss:0.865077, AvgAcc:64.9089%\n",
      "[2022-05-10 08:05:33]    Batch:  370, Loss:0.908409, AvgAcc:64.9683%\n",
      "[2022-05-10 08:06:31]    Batch:  380, Loss:0.824558, AvgAcc:65.0308%\n",
      "[2022-05-10 08:07:29]    Batch:  390, Loss:0.841202, AvgAcc:65.1062%\n",
      "Epoch:  7, learning rate = 0.1\n",
      "[2022-05-10 08:10:27]    Batch:   10, Loss:0.710501, AvgAcc:67.3438%\n",
      "[2022-05-10 08:11:18]    Batch:   20, Loss:0.840758, AvgAcc:67.1094%\n",
      "[2022-05-10 08:12:38]    Batch:   30, Loss:0.832295, AvgAcc:67.1615%\n",
      "[2022-05-10 08:13:33]    Batch:   40, Loss:0.950839, AvgAcc:67.1094%\n",
      "[2022-05-10 08:14:22]    Batch:   50, Loss:0.905873, AvgAcc:66.9219%\n",
      "[2022-05-10 08:15:14]    Batch:   60, Loss:1.028327, AvgAcc:66.8880%\n",
      "[2022-05-10 08:16:05]    Batch:   70, Loss:0.930701, AvgAcc:67.0647%\n",
      "[2022-05-10 08:16:57]    Batch:   80, Loss:1.080664, AvgAcc:67.1582%\n",
      "[2022-05-10 08:18:01]    Batch:   90, Loss:0.940495, AvgAcc:67.0486%\n",
      "[2022-05-10 08:19:02]    Batch:  100, Loss:1.004971, AvgAcc:67.0000%\n",
      "[2022-05-10 08:20:02]    Batch:  110, Loss:1.040350, AvgAcc:67.1165%\n",
      "[2022-05-10 08:20:57]    Batch:  120, Loss:0.918467, AvgAcc:67.1094%\n",
      "[2022-05-10 08:21:52]    Batch:  130, Loss:0.972271, AvgAcc:67.1394%\n",
      "[2022-05-10 08:22:49]    Batch:  140, Loss:0.807891, AvgAcc:67.3940%\n",
      "[2022-05-10 08:23:42]    Batch:  150, Loss:0.846741, AvgAcc:67.4948%\n",
      "[2022-05-10 08:24:37]    Batch:  160, Loss:0.934665, AvgAcc:67.3730%\n",
      "[2022-05-10 08:25:35]    Batch:  170, Loss:0.936519, AvgAcc:67.5138%\n",
      "[2022-05-10 08:26:30]    Batch:  180, Loss:0.868937, AvgAcc:67.5651%\n",
      "[2022-05-10 08:27:19]    Batch:  190, Loss:0.845146, AvgAcc:67.7549%\n",
      "[2022-05-10 08:28:10]    Batch:  200, Loss:0.993428, AvgAcc:67.7305%\n",
      "[2022-05-10 08:29:02]    Batch:  210, Loss:0.996811, AvgAcc:67.8348%\n",
      "[2022-05-10 08:29:59]    Batch:  220, Loss:0.799998, AvgAcc:67.8693%\n",
      "[2022-05-10 08:31:01]    Batch:  230, Loss:0.547803, AvgAcc:67.9857%\n",
      "[2022-05-10 08:31:52]    Batch:  240, Loss:0.901976, AvgAcc:68.0208%\n",
      "[2022-05-10 08:32:42]    Batch:  250, Loss:0.877604, AvgAcc:68.0250%\n",
      "[2022-05-10 08:33:44]    Batch:  260, Loss:0.778618, AvgAcc:67.9868%\n",
      "[2022-05-10 08:34:42]    Batch:  270, Loss:0.902188, AvgAcc:67.9832%\n",
      "[2022-05-10 08:35:35]    Batch:  280, Loss:0.942894, AvgAcc:68.0385%\n",
      "[2022-05-10 08:36:29]    Batch:  290, Loss:0.950429, AvgAcc:68.1196%\n",
      "[2022-05-10 08:37:21]    Batch:  300, Loss:1.008840, AvgAcc:68.1302%\n",
      "[2022-05-10 08:38:15]    Batch:  310, Loss:1.009201, AvgAcc:68.2031%\n",
      "[2022-05-10 08:39:13]    Batch:  320, Loss:0.977859, AvgAcc:68.2446%\n",
      "[2022-05-10 08:40:11]    Batch:  330, Loss:0.832931, AvgAcc:68.2599%\n",
      "[2022-05-10 08:41:09]    Batch:  340, Loss:0.942432, AvgAcc:68.2950%\n",
      "[2022-05-10 08:41:59]    Batch:  350, Loss:0.848053, AvgAcc:68.3638%\n",
      "[2022-05-10 08:42:53]    Batch:  360, Loss:0.821579, AvgAcc:68.4223%\n",
      "[2022-05-10 08:43:51]    Batch:  370, Loss:0.817688, AvgAcc:68.4818%\n",
      "[2022-05-10 08:44:45]    Batch:  380, Loss:0.784808, AvgAcc:68.5382%\n",
      "[2022-05-10 08:45:45]    Batch:  390, Loss:0.792173, AvgAcc:68.6278%\n",
      "Epoch:  8, learning rate = 0.1\n",
      "[2022-05-10 08:48:43]    Batch:   10, Loss:0.686756, AvgAcc:71.7969%\n",
      "[2022-05-10 08:49:33]    Batch:   20, Loss:0.806201, AvgAcc:71.4062%\n",
      "[2022-05-10 08:50:27]    Batch:   30, Loss:0.761530, AvgAcc:70.9635%\n",
      "[2022-05-10 08:51:26]    Batch:   40, Loss:1.052244, AvgAcc:70.3125%\n",
      "[2022-05-10 08:52:19]    Batch:   50, Loss:0.744517, AvgAcc:70.0938%\n",
      "[2022-05-10 08:53:16]    Batch:   60, Loss:0.928052, AvgAcc:70.2214%\n",
      "[2022-05-10 08:54:14]    Batch:   70, Loss:0.874901, AvgAcc:70.1228%\n",
      "[2022-05-10 08:55:08]    Batch:   80, Loss:0.967834, AvgAcc:70.2148%\n",
      "[2022-05-10 08:56:02]    Batch:   90, Loss:0.961168, AvgAcc:70.0087%\n",
      "[2022-05-10 08:56:52]    Batch:  100, Loss:0.875863, AvgAcc:70.0234%\n",
      "[2022-05-10 08:57:46]    Batch:  110, Loss:0.843686, AvgAcc:70.0142%\n",
      "[2022-05-10 08:58:43]    Batch:  120, Loss:0.806418, AvgAcc:70.0326%\n",
      "[2022-05-10 08:59:41]    Batch:  130, Loss:0.931382, AvgAcc:70.0962%\n",
      "[2022-05-10 09:00:34]    Batch:  140, Loss:0.797486, AvgAcc:70.1228%\n",
      "[2022-05-10 09:01:24]    Batch:  150, Loss:0.869588, AvgAcc:70.1354%\n",
      "[2022-05-10 09:02:13]    Batch:  160, Loss:0.804891, AvgAcc:70.1807%\n",
      "[2022-05-10 09:03:09]    Batch:  170, Loss:0.846509, AvgAcc:70.2160%\n",
      "[2022-05-10 09:04:11]    Batch:  180, Loss:0.849841, AvgAcc:70.2387%\n",
      "[2022-05-10 09:05:10]    Batch:  190, Loss:0.700361, AvgAcc:70.3413%\n",
      "[2022-05-10 09:06:09]    Batch:  200, Loss:0.754761, AvgAcc:70.3789%\n",
      "[2022-05-10 09:07:10]    Batch:  210, Loss:0.927022, AvgAcc:70.4018%\n",
      "[2022-05-10 09:08:12]    Batch:  220, Loss:0.778960, AvgAcc:70.5220%\n",
      "[2022-05-10 09:09:14]    Batch:  230, Loss:0.631394, AvgAcc:70.6284%\n",
      "[2022-05-10 09:10:16]    Batch:  240, Loss:0.902656, AvgAcc:70.7389%\n",
      "[2022-05-10 09:11:17]    Batch:  250, Loss:0.858010, AvgAcc:70.8094%\n",
      "[2022-05-10 09:12:14]    Batch:  260, Loss:0.751990, AvgAcc:70.8173%\n",
      "[2022-05-10 09:13:02]    Batch:  270, Loss:0.898054, AvgAcc:70.7986%\n",
      "[2022-05-10 09:13:57]    Batch:  280, Loss:0.798679, AvgAcc:70.7924%\n",
      "[2022-05-10 09:14:57]    Batch:  290, Loss:0.748713, AvgAcc:70.7920%\n",
      "[2022-05-10 09:15:58]    Batch:  300, Loss:0.900366, AvgAcc:70.7500%\n",
      "[2022-05-10 09:17:00]    Batch:  310, Loss:0.806350, AvgAcc:70.8165%\n",
      "[2022-05-10 09:18:00]    Batch:  320, Loss:0.873118, AvgAcc:70.8813%\n",
      "[2022-05-10 09:18:59]    Batch:  330, Loss:0.795937, AvgAcc:70.8925%\n",
      "[2022-05-10 09:19:57]    Batch:  340, Loss:0.837236, AvgAcc:70.8732%\n",
      "[2022-05-10 09:20:55]    Batch:  350, Loss:0.804321, AvgAcc:70.9196%\n",
      "[2022-05-10 09:21:55]    Batch:  360, Loss:0.732256, AvgAcc:70.9462%\n",
      "[2022-05-10 09:22:54]    Batch:  370, Loss:0.770768, AvgAcc:70.9396%\n",
      "[2022-05-10 09:23:56]    Batch:  380, Loss:0.666387, AvgAcc:71.0115%\n",
      "[2022-05-10 09:24:56]    Batch:  390, Loss:0.714008, AvgAcc:71.0837%\n",
      "Epoch:  9, learning rate = 0.1\n",
      "[2022-05-10 09:28:18]    Batch:   10, Loss:0.642098, AvgAcc:73.7500%\n",
      "[2022-05-10 09:29:17]    Batch:   20, Loss:0.826727, AvgAcc:72.6562%\n",
      "[2022-05-10 09:30:16]    Batch:   30, Loss:0.623914, AvgAcc:72.6562%\n",
      "[2022-05-10 09:31:15]    Batch:   40, Loss:0.881841, AvgAcc:72.7148%\n",
      "[2022-05-10 09:32:13]    Batch:   50, Loss:0.684292, AvgAcc:72.3125%\n",
      "[2022-05-10 09:33:12]    Batch:   60, Loss:0.898979, AvgAcc:72.2396%\n",
      "[2022-05-10 09:34:07]    Batch:   70, Loss:0.696413, AvgAcc:72.3996%\n",
      "[2022-05-10 09:35:09]    Batch:   80, Loss:0.706266, AvgAcc:72.6465%\n",
      "[2022-05-10 09:36:06]    Batch:   90, Loss:0.817790, AvgAcc:72.2830%\n",
      "[2022-05-10 09:37:03]    Batch:  100, Loss:0.788636, AvgAcc:72.4141%\n",
      "[2022-05-10 09:37:58]    Batch:  110, Loss:0.810220, AvgAcc:72.4716%\n",
      "[2022-05-10 09:38:56]    Batch:  120, Loss:0.708478, AvgAcc:72.5846%\n",
      "[2022-05-10 09:39:56]    Batch:  130, Loss:0.796818, AvgAcc:72.6442%\n",
      "[2022-05-10 09:40:55]    Batch:  140, Loss:0.745131, AvgAcc:72.7065%\n",
      "[2022-05-10 09:41:56]    Batch:  150, Loss:0.788220, AvgAcc:72.8542%\n",
      "[2022-05-10 09:42:56]    Batch:  160, Loss:0.790393, AvgAcc:72.8662%\n",
      "[2022-05-10 09:43:55]    Batch:  170, Loss:0.753721, AvgAcc:72.9963%\n",
      "[2022-05-10 09:44:55]    Batch:  180, Loss:0.749848, AvgAcc:72.9688%\n",
      "[2022-05-10 09:45:55]    Batch:  190, Loss:0.651439, AvgAcc:72.9934%\n",
      "[2022-05-10 09:46:57]    Batch:  200, Loss:0.664467, AvgAcc:72.9805%\n",
      "[2022-05-10 09:47:55]    Batch:  210, Loss:0.992578, AvgAcc:73.0171%\n",
      "[2022-05-10 09:48:54]    Batch:  220, Loss:0.799321, AvgAcc:73.0788%\n",
      "[2022-05-10 09:49:55]    Batch:  230, Loss:0.491140, AvgAcc:73.2065%\n",
      "[2022-05-10 09:50:58]    Batch:  240, Loss:0.817694, AvgAcc:73.2552%\n",
      "[2022-05-10 09:51:59]    Batch:  250, Loss:0.819230, AvgAcc:73.2719%\n",
      "[2022-05-10 09:52:59]    Batch:  260, Loss:0.666427, AvgAcc:73.2332%\n",
      "[2022-05-10 09:53:58]    Batch:  270, Loss:0.746305, AvgAcc:73.1916%\n",
      "[2022-05-10 09:54:58]    Batch:  280, Loss:0.776281, AvgAcc:73.1641%\n",
      "[2022-05-10 09:55:52]    Batch:  290, Loss:0.731927, AvgAcc:73.1735%\n",
      "[2022-05-10 09:56:45]    Batch:  300, Loss:0.777771, AvgAcc:73.1198%\n",
      "[2022-05-10 09:57:36]    Batch:  310, Loss:0.907245, AvgAcc:73.1275%\n",
      "[2022-05-10 09:58:33]    Batch:  320, Loss:0.807599, AvgAcc:73.1958%\n",
      "[2022-05-10 09:59:31]    Batch:  330, Loss:0.631215, AvgAcc:73.2647%\n",
      "[2022-05-10 10:00:25]    Batch:  340, Loss:0.761409, AvgAcc:73.3157%\n",
      "[2022-05-10 10:01:23]    Batch:  350, Loss:0.758172, AvgAcc:73.3661%\n",
      "[2022-05-10 10:02:23]    Batch:  360, Loss:0.659576, AvgAcc:73.4549%\n",
      "[2022-05-10 10:03:23]    Batch:  370, Loss:0.702077, AvgAcc:73.4481%\n",
      "[2022-05-10 10:04:22]    Batch:  380, Loss:0.546086, AvgAcc:73.4827%\n",
      "[2022-05-10 10:05:16]    Batch:  390, Loss:0.671577, AvgAcc:73.5176%\n",
      "Epoch: 10, learning rate = 0.1\n",
      "[2022-05-10 10:08:31]    Batch:   10, Loss:0.605980, AvgAcc:76.1719%\n",
      "[2022-05-10 10:09:28]    Batch:   20, Loss:0.726223, AvgAcc:74.1016%\n",
      "[2022-05-10 10:10:14]    Batch:   30, Loss:0.641840, AvgAcc:73.8281%\n",
      "[2022-05-10 10:10:33]    Batch:   40, Loss:0.923035, AvgAcc:73.7305%\n",
      "[2022-05-10 10:10:52]    Batch:   50, Loss:0.667050, AvgAcc:73.8594%\n",
      "[2022-05-10 10:11:10]    Batch:   60, Loss:0.828907, AvgAcc:73.8411%\n",
      "[2022-05-10 10:11:29]    Batch:   70, Loss:0.702817, AvgAcc:74.0290%\n",
      "[2022-05-10 10:11:50]    Batch:   80, Loss:0.846876, AvgAcc:74.0527%\n",
      "[2022-05-10 10:12:10]    Batch:   90, Loss:0.770490, AvgAcc:74.1927%\n",
      "[2022-05-10 10:12:28]    Batch:  100, Loss:0.663085, AvgAcc:74.1562%\n",
      "[2022-05-10 10:12:45]    Batch:  110, Loss:0.773565, AvgAcc:74.1193%\n",
      "[2022-05-10 10:13:02]    Batch:  120, Loss:0.693772, AvgAcc:74.2773%\n",
      "[2022-05-10 10:13:23]    Batch:  130, Loss:0.805474, AvgAcc:74.3269%\n",
      "[2022-05-10 10:13:46]    Batch:  140, Loss:0.580337, AvgAcc:74.4475%\n",
      "[2022-05-10 10:14:09]    Batch:  150, Loss:0.781725, AvgAcc:74.5208%\n",
      "[2022-05-10 10:14:27]    Batch:  160, Loss:0.722394, AvgAcc:74.5166%\n",
      "[2022-05-10 10:14:44]    Batch:  170, Loss:0.784569, AvgAcc:74.6507%\n",
      "[2022-05-10 10:15:01]    Batch:  180, Loss:0.708577, AvgAcc:74.6615%\n",
      "[2022-05-10 10:15:18]    Batch:  190, Loss:0.613457, AvgAcc:74.6669%\n",
      "[2022-05-10 10:15:35]    Batch:  200, Loss:0.686378, AvgAcc:74.6719%\n",
      "[2022-05-10 10:15:51]    Batch:  210, Loss:0.851154, AvgAcc:74.7433%\n",
      "[2022-05-10 10:16:11]    Batch:  220, Loss:0.654236, AvgAcc:74.8224%\n",
      "[2022-05-10 10:16:31]    Batch:  230, Loss:0.435170, AvgAcc:74.9592%\n",
      "[2022-05-10 10:16:51]    Batch:  240, Loss:0.628851, AvgAcc:75.0716%\n",
      "[2022-05-10 10:17:08]    Batch:  250, Loss:0.754380, AvgAcc:75.0531%\n",
      "[2022-05-10 10:17:25]    Batch:  260, Loss:0.660718, AvgAcc:75.0060%\n",
      "[2022-05-10 10:17:42]    Batch:  270, Loss:0.718581, AvgAcc:74.9508%\n",
      "[2022-05-10 10:17:59]    Batch:  280, Loss:0.794379, AvgAcc:74.9888%\n",
      "[2022-05-10 10:18:15]    Batch:  290, Loss:0.623295, AvgAcc:74.9461%\n",
      "[2022-05-10 10:18:32]    Batch:  300, Loss:0.731359, AvgAcc:74.8906%\n",
      "[2022-05-10 10:18:49]    Batch:  310, Loss:0.796275, AvgAcc:74.9294%\n",
      "[2022-05-10 10:19:06]    Batch:  320, Loss:0.816822, AvgAcc:74.9829%\n",
      "[2022-05-10 10:19:25]    Batch:  330, Loss:0.639362, AvgAcc:74.9621%\n",
      "[2022-05-10 10:19:44]    Batch:  340, Loss:0.723974, AvgAcc:74.9426%\n",
      "[2022-05-10 10:20:03]    Batch:  350, Loss:0.678954, AvgAcc:74.9866%\n",
      "[2022-05-10 10:20:22]    Batch:  360, Loss:0.588921, AvgAcc:75.0608%\n",
      "[2022-05-10 10:20:41]    Batch:  370, Loss:0.687124, AvgAcc:75.0950%\n",
      "[2022-05-10 10:20:59]    Batch:  380, Loss:0.632075, AvgAcc:75.1357%\n",
      "[2022-05-10 10:21:18]    Batch:  390, Loss:0.578082, AvgAcc:75.1923%\n",
      "Epoch: 11, learning rate = 0.1\n",
      "[2022-05-10 10:22:28]    Batch:   10, Loss:0.675084, AvgAcc:78.1250%\n",
      "[2022-05-10 10:22:50]    Batch:   20, Loss:0.658959, AvgAcc:76.8359%\n",
      "[2022-05-10 10:23:11]    Batch:   30, Loss:0.623441, AvgAcc:76.7448%\n",
      "[2022-05-10 10:23:29]    Batch:   40, Loss:0.776322, AvgAcc:76.4648%\n",
      "[2022-05-10 10:23:48]    Batch:   50, Loss:0.567416, AvgAcc:76.5938%\n",
      "[2022-05-10 10:24:07]    Batch:   60, Loss:0.766427, AvgAcc:76.3542%\n",
      "[2022-05-10 10:24:26]    Batch:   70, Loss:0.677177, AvgAcc:76.3170%\n",
      "[2022-05-10 10:24:46]    Batch:   80, Loss:0.723815, AvgAcc:76.4062%\n",
      "[2022-05-10 10:25:05]    Batch:   90, Loss:0.663330, AvgAcc:76.3281%\n",
      "[2022-05-10 10:25:24]    Batch:  100, Loss:0.626240, AvgAcc:76.2812%\n",
      "[2022-05-10 10:25:42]    Batch:  110, Loss:0.665077, AvgAcc:76.2713%\n",
      "[2022-05-10 10:26:02]    Batch:  120, Loss:0.673745, AvgAcc:76.3021%\n",
      "[2022-05-10 10:26:22]    Batch:  130, Loss:0.735395, AvgAcc:76.3942%\n",
      "[2022-05-10 10:26:41]    Batch:  140, Loss:0.635872, AvgAcc:76.4397%\n",
      "[2022-05-10 10:26:58]    Batch:  150, Loss:0.640441, AvgAcc:76.5104%\n",
      "[2022-05-10 10:27:15]    Batch:  160, Loss:0.669437, AvgAcc:76.4990%\n",
      "[2022-05-10 10:27:33]    Batch:  170, Loss:0.641607, AvgAcc:76.5257%\n",
      "[2022-05-10 10:27:52]    Batch:  180, Loss:0.709038, AvgAcc:76.4974%\n",
      "[2022-05-10 10:28:14]    Batch:  190, Loss:0.617906, AvgAcc:76.4556%\n",
      "[2022-05-10 10:28:36]    Batch:  200, Loss:0.652494, AvgAcc:76.4688%\n",
      "[2022-05-10 10:28:57]    Batch:  210, Loss:0.839639, AvgAcc:76.4993%\n",
      "[2022-05-10 10:29:13]    Batch:  220, Loss:0.565900, AvgAcc:76.5483%\n",
      "[2022-05-10 10:29:31]    Batch:  230, Loss:0.414257, AvgAcc:76.6576%\n",
      "[2022-05-10 10:29:47]    Batch:  240, Loss:0.640199, AvgAcc:76.7285%\n",
      "[2022-05-10 10:30:04]    Batch:  250, Loss:0.796244, AvgAcc:76.6750%\n",
      "[2022-05-10 10:30:22]    Batch:  260, Loss:0.630455, AvgAcc:76.6466%\n",
      "[2022-05-10 10:30:42]    Batch:  270, Loss:0.683303, AvgAcc:76.5596%\n",
      "[2022-05-10 10:31:02]    Batch:  280, Loss:0.697111, AvgAcc:76.5653%\n",
      "[2022-05-10 10:31:20]    Batch:  290, Loss:0.563415, AvgAcc:76.5787%\n",
      "[2022-05-10 10:31:36]    Batch:  300, Loss:0.762540, AvgAcc:76.5391%\n",
      "[2022-05-10 10:31:53]    Batch:  310, Loss:0.829872, AvgAcc:76.5549%\n",
      "[2022-05-10 10:32:10]    Batch:  320, Loss:0.760944, AvgAcc:76.5454%\n",
      "[2022-05-10 10:32:27]    Batch:  330, Loss:0.487607, AvgAcc:76.6264%\n",
      "[2022-05-10 10:32:43]    Batch:  340, Loss:0.701665, AvgAcc:76.6498%\n",
      "[2022-05-10 10:33:00]    Batch:  350, Loss:0.709803, AvgAcc:76.6562%\n",
      "[2022-05-10 10:33:17]    Batch:  360, Loss:0.618966, AvgAcc:76.6753%\n",
      "[2022-05-10 10:33:36]    Batch:  370, Loss:0.622161, AvgAcc:76.7145%\n",
      "[2022-05-10 10:33:55]    Batch:  380, Loss:0.579546, AvgAcc:76.7866%\n",
      "[2022-05-10 10:34:16]    Batch:  390, Loss:0.568798, AvgAcc:76.8550%\n",
      "Epoch: 12, learning rate = 0.1\n",
      "[2022-05-10 10:35:24]    Batch:   10, Loss:0.447077, AvgAcc:80.6250%\n",
      "[2022-05-10 10:35:43]    Batch:   20, Loss:0.536858, AvgAcc:79.6484%\n",
      "[2022-05-10 10:36:02]    Batch:   30, Loss:0.573004, AvgAcc:79.5052%\n",
      "[2022-05-10 10:36:21]    Batch:   40, Loss:0.873898, AvgAcc:78.7695%\n",
      "[2022-05-10 10:36:41]    Batch:   50, Loss:0.592583, AvgAcc:78.2969%\n",
      "[2022-05-10 10:37:03]    Batch:   60, Loss:0.713660, AvgAcc:78.0469%\n",
      "[2022-05-10 10:37:25]    Batch:   70, Loss:0.536309, AvgAcc:78.3036%\n",
      "[2022-05-10 10:37:47]    Batch:   80, Loss:0.676401, AvgAcc:78.2031%\n",
      "[2022-05-10 10:38:05]    Batch:   90, Loss:0.663584, AvgAcc:78.1684%\n",
      "[2022-05-10 10:38:22]    Batch:  100, Loss:0.618585, AvgAcc:78.0469%\n",
      "[2022-05-10 10:38:41]    Batch:  110, Loss:0.654460, AvgAcc:77.9474%\n",
      "[2022-05-10 10:39:01]    Batch:  120, Loss:0.656978, AvgAcc:77.8516%\n",
      "[2022-05-10 10:39:19]    Batch:  130, Loss:0.732123, AvgAcc:77.9988%\n",
      "[2022-05-10 10:39:38]    Batch:  140, Loss:0.600365, AvgAcc:77.9408%\n",
      "[2022-05-10 10:39:57]    Batch:  150, Loss:0.649742, AvgAcc:78.0365%\n",
      "[2022-05-10 10:40:17]    Batch:  160, Loss:0.646341, AvgAcc:78.0713%\n",
      "[2022-05-10 10:40:35]    Batch:  170, Loss:0.687480, AvgAcc:78.0607%\n",
      "[2022-05-10 10:40:56]    Batch:  180, Loss:0.575604, AvgAcc:78.0599%\n",
      "[2022-05-10 10:41:16]    Batch:  190, Loss:0.455467, AvgAcc:78.0510%\n",
      "[2022-05-10 10:41:36]    Batch:  200, Loss:0.607197, AvgAcc:78.0625%\n",
      "[2022-05-10 10:41:53]    Batch:  210, Loss:0.780749, AvgAcc:78.0655%\n",
      "[2022-05-10 10:42:10]    Batch:  220, Loss:0.506096, AvgAcc:78.0788%\n",
      "[2022-05-10 10:42:27]    Batch:  230, Loss:0.382041, AvgAcc:78.1692%\n",
      "[2022-05-10 10:42:46]    Batch:  240, Loss:0.670043, AvgAcc:78.2520%\n",
      "[2022-05-10 10:43:06]    Batch:  250, Loss:0.632218, AvgAcc:78.2375%\n",
      "[2022-05-10 10:43:28]    Batch:  260, Loss:0.492571, AvgAcc:78.2061%\n",
      "[2022-05-10 10:43:48]    Batch:  270, Loss:0.751225, AvgAcc:78.1626%\n",
      "[2022-05-10 10:44:06]    Batch:  280, Loss:0.633898, AvgAcc:78.1250%\n",
      "[2022-05-10 10:44:23]    Batch:  290, Loss:0.621960, AvgAcc:78.1169%\n",
      "[2022-05-10 10:44:39]    Batch:  300, Loss:0.785995, AvgAcc:78.0911%\n",
      "[2022-05-10 10:44:56]    Batch:  310, Loss:0.733436, AvgAcc:78.0595%\n",
      "[2022-05-10 10:45:14]    Batch:  320, Loss:0.709773, AvgAcc:78.0811%\n",
      "[2022-05-10 10:45:34]    Batch:  330, Loss:0.480102, AvgAcc:78.1368%\n",
      "[2022-05-10 10:45:54]    Batch:  340, Loss:0.634546, AvgAcc:78.1319%\n",
      "[2022-05-10 10:46:13]    Batch:  350, Loss:0.543255, AvgAcc:78.2076%\n",
      "[2022-05-10 10:46:30]    Batch:  360, Loss:0.563137, AvgAcc:78.2595%\n",
      "[2022-05-10 10:46:47]    Batch:  370, Loss:0.605308, AvgAcc:78.2432%\n",
      "[2022-05-10 10:47:04]    Batch:  380, Loss:0.467464, AvgAcc:78.2792%\n",
      "[2022-05-10 10:47:21]    Batch:  390, Loss:0.480459, AvgAcc:78.3914%\n",
      "Epoch: 13, learning rate = 0.1\n",
      "[2022-05-10 10:48:22]    Batch:   10, Loss:0.569027, AvgAcc:80.7812%\n",
      "[2022-05-10 10:48:42]    Batch:   20, Loss:0.505924, AvgAcc:80.2344%\n",
      "[2022-05-10 10:49:03]    Batch:   30, Loss:0.506805, AvgAcc:79.7656%\n",
      "[2022-05-10 10:49:22]    Batch:   40, Loss:0.726550, AvgAcc:79.7266%\n",
      "[2022-05-10 10:49:41]    Batch:   50, Loss:0.562198, AvgAcc:79.3906%\n",
      "[2022-05-10 10:49:59]    Batch:   60, Loss:0.587488, AvgAcc:79.3229%\n",
      "[2022-05-10 10:50:18]    Batch:   70, Loss:0.600505, AvgAcc:79.2857%\n",
      "[2022-05-10 10:50:38]    Batch:   80, Loss:0.618998, AvgAcc:79.4531%\n",
      "[2022-05-10 10:50:59]    Batch:   90, Loss:0.582641, AvgAcc:79.3316%\n",
      "[2022-05-10 10:51:18]    Batch:  100, Loss:0.627672, AvgAcc:79.3516%\n",
      "[2022-05-10 10:51:37]    Batch:  110, Loss:0.627220, AvgAcc:79.4176%\n",
      "[2022-05-10 10:51:57]    Batch:  120, Loss:0.622821, AvgAcc:79.3620%\n",
      "[2022-05-10 10:52:18]    Batch:  130, Loss:0.651556, AvgAcc:79.3630%\n",
      "[2022-05-10 10:52:39]    Batch:  140, Loss:0.503769, AvgAcc:79.4364%\n",
      "[2022-05-10 10:53:00]    Batch:  150, Loss:0.677409, AvgAcc:79.4115%\n",
      "[2022-05-10 10:53:19]    Batch:  160, Loss:0.642223, AvgAcc:79.3262%\n",
      "[2022-05-10 10:53:38]    Batch:  170, Loss:0.727925, AvgAcc:79.3153%\n",
      "[2022-05-10 10:53:56]    Batch:  180, Loss:0.635231, AvgAcc:79.3533%\n",
      "[2022-05-10 10:54:15]    Batch:  190, Loss:0.524780, AvgAcc:79.3709%\n",
      "[2022-05-10 10:54:34]    Batch:  200, Loss:0.613231, AvgAcc:79.3945%\n",
      "[2022-05-10 10:54:53]    Batch:  210, Loss:0.780918, AvgAcc:79.3899%\n",
      "[2022-05-10 10:55:13]    Batch:  220, Loss:0.426612, AvgAcc:79.4283%\n",
      "[2022-05-10 10:55:31]    Batch:  230, Loss:0.413709, AvgAcc:79.4871%\n",
      "[2022-05-10 10:55:51]    Batch:  240, Loss:0.588735, AvgAcc:79.5736%\n",
      "[2022-05-10 10:56:12]    Batch:  250, Loss:0.776555, AvgAcc:79.5406%\n",
      "[2022-05-10 10:56:32]    Batch:  260, Loss:0.523326, AvgAcc:79.4441%\n",
      "[2022-05-10 10:56:48]    Batch:  270, Loss:0.685977, AvgAcc:79.4068%\n",
      "[2022-05-10 10:57:04]    Batch:  280, Loss:0.608748, AvgAcc:79.3945%\n",
      "[2022-05-10 10:57:20]    Batch:  290, Loss:0.553276, AvgAcc:79.3723%\n",
      "[2022-05-10 10:57:38]    Batch:  300, Loss:0.592952, AvgAcc:79.3672%\n",
      "[2022-05-10 10:57:58]    Batch:  310, Loss:0.738421, AvgAcc:79.3826%\n",
      "[2022-05-10 10:58:20]    Batch:  320, Loss:0.617809, AvgAcc:79.4067%\n",
      "[2022-05-10 10:58:40]    Batch:  330, Loss:0.482272, AvgAcc:79.4389%\n",
      "[2022-05-10 10:59:00]    Batch:  340, Loss:0.680066, AvgAcc:79.4003%\n",
      "[2022-05-10 10:59:18]    Batch:  350, Loss:0.635226, AvgAcc:79.4040%\n",
      "[2022-05-10 10:59:34]    Batch:  360, Loss:0.587772, AvgAcc:79.4227%\n",
      "[2022-05-10 10:59:51]    Batch:  370, Loss:0.644023, AvgAcc:79.4320%\n",
      "[2022-05-10 11:00:09]    Batch:  380, Loss:0.465090, AvgAcc:79.4613%\n",
      "[2022-05-10 11:00:29]    Batch:  390, Loss:0.456310, AvgAcc:79.5152%\n",
      "Epoch: 14, learning rate = 0.1\n",
      "[2022-05-10 11:01:33]    Batch:   10, Loss:0.506840, AvgAcc:81.2500%\n",
      "[2022-05-10 11:01:50]    Batch:   20, Loss:0.492709, AvgAcc:81.0547%\n",
      "[2022-05-10 11:02:06]    Batch:   30, Loss:0.444688, AvgAcc:80.8073%\n",
      "[2022-05-10 11:02:23]    Batch:   40, Loss:0.786720, AvgAcc:81.0742%\n",
      "[2022-05-10 11:02:39]    Batch:   50, Loss:0.431194, AvgAcc:81.1562%\n",
      "[2022-05-10 11:02:56]    Batch:   60, Loss:0.575473, AvgAcc:80.8854%\n",
      "[2022-05-10 11:03:13]    Batch:   70, Loss:0.504389, AvgAcc:80.8147%\n",
      "[2022-05-10 11:03:33]    Batch:   80, Loss:0.552180, AvgAcc:80.7324%\n",
      "[2022-05-10 11:03:53]    Batch:   90, Loss:0.576065, AvgAcc:80.5816%\n",
      "[2022-05-10 11:04:15]    Batch:  100, Loss:0.630765, AvgAcc:80.6641%\n",
      "[2022-05-10 11:04:35]    Batch:  110, Loss:0.545035, AvgAcc:80.6534%\n",
      "[2022-05-10 11:04:54]    Batch:  120, Loss:0.540565, AvgAcc:80.6315%\n",
      "[2022-05-10 11:05:13]    Batch:  130, Loss:0.667795, AvgAcc:80.6550%\n",
      "[2022-05-10 11:05:32]    Batch:  140, Loss:0.594052, AvgAcc:80.5915%\n",
      "[2022-05-10 11:05:50]    Batch:  150, Loss:0.561872, AvgAcc:80.5156%\n",
      "[2022-05-10 11:06:09]    Batch:  160, Loss:0.566905, AvgAcc:80.5762%\n",
      "[2022-05-10 11:06:29]    Batch:  170, Loss:0.564739, AvgAcc:80.6250%\n",
      "[2022-05-10 11:06:49]    Batch:  180, Loss:0.544183, AvgAcc:80.5729%\n",
      "[2022-05-10 11:07:11]    Batch:  190, Loss:0.462083, AvgAcc:80.5345%\n",
      "[2022-05-10 11:07:33]    Batch:  200, Loss:0.528540, AvgAcc:80.5391%\n",
      "[2022-05-10 11:07:54]    Batch:  210, Loss:0.716380, AvgAcc:80.5618%\n",
      "[2022-05-10 11:08:12]    Batch:  220, Loss:0.473668, AvgAcc:80.5930%\n",
      "[2022-05-10 11:08:30]    Batch:  230, Loss:0.348404, AvgAcc:80.6692%\n",
      "[2022-05-10 11:08:48]    Batch:  240, Loss:0.591951, AvgAcc:80.7357%\n",
      "[2022-05-10 11:09:07]    Batch:  250, Loss:0.665941, AvgAcc:80.7375%\n",
      "[2022-05-10 11:09:26]    Batch:  260, Loss:0.578584, AvgAcc:80.7031%\n",
      "[2022-05-10 11:09:45]    Batch:  270, Loss:0.596328, AvgAcc:80.6771%\n",
      "[2022-05-10 11:10:03]    Batch:  280, Loss:0.557702, AvgAcc:80.7031%\n",
      "[2022-05-10 11:10:20]    Batch:  290, Loss:0.577473, AvgAcc:80.6546%\n",
      "[2022-05-10 11:10:38]    Batch:  300, Loss:0.585081, AvgAcc:80.6120%\n",
      "[2022-05-10 11:10:58]    Batch:  310, Loss:0.662067, AvgAcc:80.6704%\n",
      "[2022-05-10 11:11:18]    Batch:  320, Loss:0.581409, AvgAcc:80.6836%\n",
      "[2022-05-10 11:11:37]    Batch:  330, Loss:0.431819, AvgAcc:80.7315%\n",
      "[2022-05-10 11:11:54]    Batch:  340, Loss:0.545286, AvgAcc:80.7031%\n",
      "[2022-05-10 11:12:11]    Batch:  350, Loss:0.556020, AvgAcc:80.6942%\n",
      "[2022-05-10 11:12:30]    Batch:  360, Loss:0.532859, AvgAcc:80.7205%\n",
      "[2022-05-10 11:12:49]    Batch:  370, Loss:0.416458, AvgAcc:80.7580%\n",
      "[2022-05-10 11:13:10]    Batch:  380, Loss:0.433444, AvgAcc:80.7998%\n",
      "[2022-05-10 11:13:31]    Batch:  390, Loss:0.462446, AvgAcc:80.8393%\n",
      "Epoch: 15, learning rate = 0.1\n",
      "[2022-05-10 11:14:35]    Batch:   10, Loss:0.503384, AvgAcc:82.6562%\n",
      "[2022-05-10 11:14:51]    Batch:   20, Loss:0.362298, AvgAcc:81.9141%\n",
      "[2022-05-10 11:15:09]    Batch:   30, Loss:0.518866, AvgAcc:81.6927%\n",
      "[2022-05-10 11:15:28]    Batch:   40, Loss:0.647660, AvgAcc:81.4844%\n",
      "[2022-05-10 11:15:48]    Batch:   50, Loss:0.372956, AvgAcc:81.3125%\n",
      "[2022-05-10 11:16:08]    Batch:   60, Loss:0.640098, AvgAcc:81.2109%\n",
      "[2022-05-10 11:16:25]    Batch:   70, Loss:0.490408, AvgAcc:81.2946%\n",
      "[2022-05-10 11:16:42]    Batch:   80, Loss:0.553863, AvgAcc:81.3281%\n",
      "[2022-05-10 11:16:58]    Batch:   90, Loss:0.553415, AvgAcc:81.2674%\n",
      "[2022-05-10 11:17:15]    Batch:  100, Loss:0.524097, AvgAcc:81.1641%\n",
      "[2022-05-10 11:17:32]    Batch:  110, Loss:0.527234, AvgAcc:81.1293%\n",
      "[2022-05-10 11:17:49]    Batch:  120, Loss:0.588065, AvgAcc:81.0677%\n",
      "[2022-05-10 11:18:06]    Batch:  130, Loss:0.605741, AvgAcc:81.1058%\n",
      "[2022-05-10 11:18:26]    Batch:  140, Loss:0.496214, AvgAcc:81.0882%\n",
      "[2022-05-10 11:18:46]    Batch:  150, Loss:0.474387, AvgAcc:81.0938%\n",
      "[2022-05-10 11:19:06]    Batch:  160, Loss:0.541946, AvgAcc:81.0645%\n",
      "[2022-05-10 11:19:25]    Batch:  170, Loss:0.677466, AvgAcc:81.0754%\n",
      "[2022-05-10 11:19:43]    Batch:  180, Loss:0.607466, AvgAcc:81.1502%\n",
      "[2022-05-10 11:20:02]    Batch:  190, Loss:0.474492, AvgAcc:81.1513%\n",
      "[2022-05-10 11:20:21]    Batch:  200, Loss:0.521185, AvgAcc:81.1641%\n",
      "[2022-05-10 11:20:40]    Batch:  210, Loss:0.775909, AvgAcc:81.2202%\n",
      "[2022-05-10 11:20:59]    Batch:  220, Loss:0.460298, AvgAcc:81.2784%\n",
      "[2022-05-10 11:21:17]    Batch:  230, Loss:0.363539, AvgAcc:81.3213%\n",
      "[2022-05-10 11:21:36]    Batch:  240, Loss:0.481015, AvgAcc:81.3574%\n",
      "[2022-05-10 11:21:59]    Batch:  250, Loss:0.571486, AvgAcc:81.3250%\n",
      "[2022-05-10 11:22:21]    Batch:  260, Loss:0.574110, AvgAcc:81.3041%\n",
      "[2022-05-10 11:22:43]    Batch:  270, Loss:0.554738, AvgAcc:81.2847%\n",
      "[2022-05-10 11:23:02]    Batch:  280, Loss:0.463414, AvgAcc:81.2779%\n",
      "[2022-05-10 11:23:19]    Batch:  290, Loss:0.481487, AvgAcc:81.2500%\n",
      "[2022-05-10 11:23:38]    Batch:  300, Loss:0.565900, AvgAcc:81.2578%\n",
      "[2022-05-10 11:23:56]    Batch:  310, Loss:0.657591, AvgAcc:81.2576%\n",
      "[2022-05-10 11:24:15]    Batch:  320, Loss:0.544823, AvgAcc:81.2671%\n",
      "[2022-05-10 11:24:35]    Batch:  330, Loss:0.526376, AvgAcc:81.2784%\n",
      "[2022-05-10 11:24:54]    Batch:  340, Loss:0.522314, AvgAcc:81.2822%\n",
      "[2022-05-10 11:25:14]    Batch:  350, Loss:0.496753, AvgAcc:81.3281%\n",
      "[2022-05-10 11:25:32]    Batch:  360, Loss:0.502096, AvgAcc:81.3390%\n",
      "[2022-05-10 11:25:52]    Batch:  370, Loss:0.425071, AvgAcc:81.3978%\n",
      "[2022-05-10 11:26:12]    Batch:  380, Loss:0.429615, AvgAcc:81.4289%\n",
      "[2022-05-10 11:26:32]    Batch:  390, Loss:0.428701, AvgAcc:81.4603%\n",
      "Epoch: 16, learning rate = 0.1\n",
      "[2022-05-10 11:27:33]    Batch:   10, Loss:0.459460, AvgAcc:83.3594%\n",
      "[2022-05-10 11:27:52]    Batch:   20, Loss:0.486112, AvgAcc:82.6172%\n",
      "[2022-05-10 11:28:11]    Batch:   30, Loss:0.464394, AvgAcc:82.9688%\n",
      "[2022-05-10 11:28:30]    Batch:   40, Loss:0.674740, AvgAcc:83.0273%\n",
      "[2022-05-10 11:28:48]    Batch:   50, Loss:0.413524, AvgAcc:82.7656%\n",
      "[2022-05-10 11:29:06]    Batch:   60, Loss:0.527715, AvgAcc:82.8255%\n",
      "[2022-05-10 11:29:26]    Batch:   70, Loss:0.490635, AvgAcc:82.7567%\n",
      "[2022-05-10 11:29:46]    Batch:   80, Loss:0.574094, AvgAcc:82.5293%\n",
      "[2022-05-10 11:30:06]    Batch:   90, Loss:0.515501, AvgAcc:82.3872%\n",
      "[2022-05-10 11:30:23]    Batch:  100, Loss:0.463451, AvgAcc:82.3203%\n",
      "[2022-05-10 11:30:43]    Batch:  110, Loss:0.522028, AvgAcc:82.1449%\n",
      "[2022-05-10 11:31:03]    Batch:  120, Loss:0.530428, AvgAcc:82.1680%\n",
      "[2022-05-10 11:31:23]    Batch:  130, Loss:0.468574, AvgAcc:82.2236%\n",
      "[2022-05-10 11:31:40]    Batch:  140, Loss:0.515945, AvgAcc:82.2321%\n",
      "[2022-05-10 11:31:57]    Batch:  150, Loss:0.622400, AvgAcc:82.1042%\n",
      "[2022-05-10 11:32:13]    Batch:  160, Loss:0.523884, AvgAcc:82.1631%\n",
      "[2022-05-10 11:32:30]    Batch:  170, Loss:0.588008, AvgAcc:82.0956%\n",
      "[2022-05-10 11:32:47]    Batch:  180, Loss:0.530700, AvgAcc:82.1354%\n",
      "[2022-05-10 11:33:04]    Batch:  190, Loss:0.404178, AvgAcc:82.1546%\n",
      "[2022-05-10 11:33:24]    Batch:  200, Loss:0.486613, AvgAcc:82.1836%\n",
      "[2022-05-10 11:33:44]    Batch:  210, Loss:0.745804, AvgAcc:82.1949%\n",
      "[2022-05-10 11:34:03]    Batch:  220, Loss:0.479173, AvgAcc:82.2408%\n",
      "[2022-05-10 11:34:22]    Batch:  230, Loss:0.337173, AvgAcc:82.3030%\n",
      "[2022-05-10 11:34:41]    Batch:  240, Loss:0.455039, AvgAcc:82.4154%\n",
      "[2022-05-10 11:35:00]    Batch:  250, Loss:0.558373, AvgAcc:82.3438%\n",
      "[2022-05-10 11:35:18]    Batch:  260, Loss:0.385448, AvgAcc:82.3137%\n",
      "[2022-05-10 11:35:36]    Batch:  270, Loss:0.554610, AvgAcc:82.2772%\n",
      "[2022-05-10 11:35:53]    Batch:  280, Loss:0.482548, AvgAcc:82.2796%\n",
      "[2022-05-10 11:36:11]    Batch:  290, Loss:0.567314, AvgAcc:82.2225%\n",
      "[2022-05-10 11:36:30]    Batch:  300, Loss:0.551900, AvgAcc:82.1875%\n",
      "[2022-05-10 11:36:49]    Batch:  310, Loss:0.579111, AvgAcc:82.1346%\n",
      "[2022-05-10 11:37:08]    Batch:  320, Loss:0.607911, AvgAcc:82.1362%\n",
      "[2022-05-10 11:37:28]    Batch:  330, Loss:0.453428, AvgAcc:82.1804%\n",
      "[2022-05-10 11:37:48]    Batch:  340, Loss:0.472947, AvgAcc:82.1760%\n",
      "[2022-05-10 11:38:11]    Batch:  350, Loss:0.510940, AvgAcc:82.1786%\n",
      "[2022-05-10 11:38:33]    Batch:  360, Loss:0.426821, AvgAcc:82.2244%\n",
      "[2022-05-10 11:38:56]    Batch:  370, Loss:0.440580, AvgAcc:82.2340%\n",
      "[2022-05-10 11:39:14]    Batch:  380, Loss:0.417419, AvgAcc:82.2553%\n",
      "[2022-05-10 11:39:34]    Batch:  390, Loss:0.468274, AvgAcc:82.2736%\n",
      "Epoch: 17, learning rate = 0.1\n",
      "[2022-05-10 11:40:39]    Batch:   10, Loss:0.429113, AvgAcc:84.6875%\n",
      "[2022-05-10 11:40:59]    Batch:   20, Loss:0.464527, AvgAcc:84.0625%\n",
      "[2022-05-10 11:41:19]    Batch:   30, Loss:0.378186, AvgAcc:83.7500%\n",
      "[2022-05-10 11:41:39]    Batch:   40, Loss:0.691311, AvgAcc:83.2031%\n",
      "[2022-05-10 11:41:55]    Batch:   50, Loss:0.464337, AvgAcc:82.9531%\n",
      "[2022-05-10 11:42:12]    Batch:   60, Loss:0.423989, AvgAcc:82.7734%\n",
      "[2022-05-10 11:42:29]    Batch:   70, Loss:0.407062, AvgAcc:82.8237%\n",
      "[2022-05-10 11:42:48]    Batch:   80, Loss:0.537270, AvgAcc:82.9102%\n",
      "[2022-05-10 11:43:07]    Batch:   90, Loss:0.437085, AvgAcc:82.6649%\n",
      "[2022-05-10 11:43:26]    Batch:  100, Loss:0.465392, AvgAcc:82.6016%\n",
      "[2022-05-10 11:43:44]    Batch:  110, Loss:0.461677, AvgAcc:82.5355%\n",
      "[2022-05-10 11:44:02]    Batch:  120, Loss:0.562278, AvgAcc:82.6302%\n",
      "[2022-05-10 11:44:21]    Batch:  130, Loss:0.498704, AvgAcc:82.6683%\n",
      "[2022-05-10 11:44:42]    Batch:  140, Loss:0.574271, AvgAcc:82.6674%\n",
      "[2022-05-10 11:45:03]    Batch:  150, Loss:0.617385, AvgAcc:82.6667%\n",
      "[2022-05-10 11:45:22]    Batch:  160, Loss:0.501974, AvgAcc:82.7246%\n",
      "[2022-05-10 11:45:43]    Batch:  170, Loss:0.625227, AvgAcc:82.7252%\n",
      "[2022-05-10 11:46:03]    Batch:  180, Loss:0.482922, AvgAcc:82.7821%\n",
      "[2022-05-10 11:46:24]    Batch:  190, Loss:0.485714, AvgAcc:82.7385%\n",
      "[2022-05-10 11:46:42]    Batch:  200, Loss:0.505873, AvgAcc:82.7383%\n",
      "[2022-05-10 11:47:01]    Batch:  210, Loss:0.587423, AvgAcc:82.7939%\n",
      "[2022-05-10 11:47:19]    Batch:  220, Loss:0.419299, AvgAcc:82.8161%\n",
      "[2022-05-10 11:47:37]    Batch:  230, Loss:0.326734, AvgAcc:82.8974%\n",
      "[2022-05-10 11:47:54]    Batch:  240, Loss:0.458795, AvgAcc:82.9557%\n",
      "[2022-05-10 11:48:12]    Batch:  250, Loss:0.513810, AvgAcc:82.9250%\n",
      "[2022-05-10 11:48:28]    Batch:  260, Loss:0.442928, AvgAcc:82.9087%\n",
      "[2022-05-10 11:48:45]    Batch:  270, Loss:0.611286, AvgAcc:82.8270%\n",
      "[2022-05-10 11:49:02]    Batch:  280, Loss:0.448802, AvgAcc:82.8376%\n",
      "[2022-05-10 11:49:23]    Batch:  290, Loss:0.522034, AvgAcc:82.8314%\n",
      "[2022-05-10 11:49:46]    Batch:  300, Loss:0.465715, AvgAcc:82.8385%\n",
      "[2022-05-10 11:50:09]    Batch:  310, Loss:0.583783, AvgAcc:82.8982%\n",
      "[2022-05-10 11:50:30]    Batch:  320, Loss:0.610589, AvgAcc:82.9004%\n",
      "[2022-05-10 11:50:50]    Batch:  330, Loss:0.401004, AvgAcc:82.9238%\n",
      "[2022-05-10 11:51:10]    Batch:  340, Loss:0.509462, AvgAcc:82.9067%\n",
      "[2022-05-10 11:51:31]    Batch:  350, Loss:0.510914, AvgAcc:82.9263%\n",
      "[2022-05-10 11:51:52]    Batch:  360, Loss:0.490452, AvgAcc:82.9405%\n",
      "[2022-05-10 11:52:13]    Batch:  370, Loss:0.510537, AvgAcc:82.9413%\n",
      "[2022-05-10 11:52:34]    Batch:  380, Loss:0.409956, AvgAcc:82.9502%\n",
      "[2022-05-10 11:52:54]    Batch:  390, Loss:0.390673, AvgAcc:82.9647%\n",
      "Epoch: 18, learning rate = 0.1\n",
      "[2022-05-10 11:54:10]    Batch:   10, Loss:0.350290, AvgAcc:84.6875%\n",
      "[2022-05-10 11:54:29]    Batch:   20, Loss:0.407313, AvgAcc:84.5312%\n",
      "[2022-05-10 11:54:48]    Batch:   30, Loss:0.372027, AvgAcc:84.0885%\n",
      "[2022-05-10 11:55:07]    Batch:   40, Loss:0.550237, AvgAcc:83.8672%\n",
      "[2022-05-10 11:55:27]    Batch:   50, Loss:0.404466, AvgAcc:83.7188%\n",
      "[2022-05-10 11:55:53]    Batch:   60, Loss:0.518864, AvgAcc:83.6719%\n",
      "[2022-05-10 11:56:18]    Batch:   70, Loss:0.441304, AvgAcc:83.8839%\n",
      "[2022-05-10 11:56:40]    Batch:   80, Loss:0.482888, AvgAcc:83.9648%\n",
      "[2022-05-10 11:56:59]    Batch:   90, Loss:0.536971, AvgAcc:83.8455%\n",
      "[2022-05-10 11:57:17]    Batch:  100, Loss:0.404383, AvgAcc:83.8594%\n",
      "[2022-05-10 11:57:35]    Batch:  110, Loss:0.453679, AvgAcc:83.7784%\n",
      "[2022-05-10 11:57:53]    Batch:  120, Loss:0.553416, AvgAcc:83.7305%\n",
      "[2022-05-10 11:58:12]    Batch:  130, Loss:0.513275, AvgAcc:83.7560%\n",
      "[2022-05-10 11:58:31]    Batch:  140, Loss:0.391930, AvgAcc:83.8337%\n",
      "[2022-05-10 11:58:54]    Batch:  150, Loss:0.487443, AvgAcc:83.8177%\n",
      "[2022-05-10 11:59:17]    Batch:  160, Loss:0.508586, AvgAcc:83.7207%\n",
      "[2022-05-10 11:59:38]    Batch:  170, Loss:0.649430, AvgAcc:83.6443%\n",
      "[2022-05-10 11:59:55]    Batch:  180, Loss:0.585478, AvgAcc:83.5286%\n",
      "[2022-05-10 12:00:12]    Batch:  190, Loss:0.346425, AvgAcc:83.5567%\n",
      "[2022-05-10 12:00:30]    Batch:  200, Loss:0.557268, AvgAcc:83.5312%\n",
      "[2022-05-10 12:00:50]    Batch:  210, Loss:0.582713, AvgAcc:83.5231%\n",
      "[2022-05-10 12:01:11]    Batch:  220, Loss:0.380494, AvgAcc:83.5795%\n",
      "[2022-05-10 12:01:31]    Batch:  230, Loss:0.325713, AvgAcc:83.5836%\n",
      "[2022-05-10 12:01:48]    Batch:  240, Loss:0.547653, AvgAcc:83.6296%\n",
      "[2022-05-10 12:02:04]    Batch:  250, Loss:0.634344, AvgAcc:83.5312%\n",
      "[2022-05-10 12:02:23]    Batch:  260, Loss:0.486779, AvgAcc:83.4435%\n",
      "[2022-05-10 12:02:40]    Batch:  270, Loss:0.514129, AvgAcc:83.4201%\n",
      "[2022-05-10 12:02:58]    Batch:  280, Loss:0.469534, AvgAcc:83.4096%\n",
      "[2022-05-10 12:03:17]    Batch:  290, Loss:0.467519, AvgAcc:83.3971%\n",
      "[2022-05-10 12:03:35]    Batch:  300, Loss:0.497730, AvgAcc:83.3698%\n",
      "[2022-05-10 12:03:52]    Batch:  310, Loss:0.461552, AvgAcc:83.3921%\n",
      "[2022-05-10 12:04:09]    Batch:  320, Loss:0.670437, AvgAcc:83.3618%\n",
      "[2022-05-10 12:04:28]    Batch:  330, Loss:0.390855, AvgAcc:83.4162%\n",
      "[2022-05-10 12:04:49]    Batch:  340, Loss:0.509090, AvgAcc:83.4168%\n",
      "[2022-05-10 12:05:11]    Batch:  350, Loss:0.580288, AvgAcc:83.4330%\n",
      "[2022-05-10 12:05:33]    Batch:  360, Loss:0.510193, AvgAcc:83.4484%\n",
      "[2022-05-10 12:05:51]    Batch:  370, Loss:0.379123, AvgAcc:83.4481%\n",
      "[2022-05-10 12:06:10]    Batch:  380, Loss:0.413123, AvgAcc:83.4889%\n",
      "[2022-05-10 12:06:28]    Batch:  390, Loss:0.296552, AvgAcc:83.5417%\n",
      "Epoch: 19, learning rate = 0.1\n",
      "[2022-05-10 12:07:42]    Batch:   10, Loss:0.359143, AvgAcc:86.4844%\n",
      "[2022-05-10 12:08:07]    Batch:   20, Loss:0.408751, AvgAcc:85.3906%\n",
      "[2022-05-10 12:08:33]    Batch:   30, Loss:0.406103, AvgAcc:85.1302%\n",
      "[2022-05-10 12:08:58]    Batch:   40, Loss:0.656538, AvgAcc:84.9805%\n",
      "[2022-05-10 12:09:20]    Batch:   50, Loss:0.396342, AvgAcc:84.7188%\n",
      "[2022-05-10 12:09:43]    Batch:   60, Loss:0.423825, AvgAcc:84.5703%\n",
      "[2022-05-10 12:10:04]    Batch:   70, Loss:0.473846, AvgAcc:84.4866%\n",
      "[2022-05-10 12:10:24]    Batch:   80, Loss:0.547966, AvgAcc:84.2676%\n",
      "[2022-05-10 12:10:45]    Batch:   90, Loss:0.564156, AvgAcc:84.0625%\n",
      "[2022-05-10 12:11:07]    Batch:  100, Loss:0.457516, AvgAcc:84.0156%\n",
      "[2022-05-10 12:11:28]    Batch:  110, Loss:0.358282, AvgAcc:83.9915%\n",
      "[2022-05-10 12:11:47]    Batch:  120, Loss:0.522111, AvgAcc:83.9583%\n",
      "[2022-05-10 12:12:04]    Batch:  130, Loss:0.498325, AvgAcc:83.9724%\n",
      "[2022-05-10 12:12:21]    Batch:  140, Loss:0.467750, AvgAcc:83.9118%\n",
      "[2022-05-10 12:12:38]    Batch:  150, Loss:0.446829, AvgAcc:83.9635%\n",
      "[2022-05-10 12:12:58]    Batch:  160, Loss:0.478406, AvgAcc:83.9355%\n",
      "[2022-05-10 12:13:17]    Batch:  170, Loss:0.469305, AvgAcc:83.8695%\n",
      "[2022-05-10 12:13:36]    Batch:  180, Loss:0.500441, AvgAcc:83.8932%\n",
      "[2022-05-10 12:13:53]    Batch:  190, Loss:0.439451, AvgAcc:83.8939%\n",
      "[2022-05-10 12:14:13]    Batch:  200, Loss:0.419446, AvgAcc:83.9961%\n",
      "[2022-05-10 12:14:33]    Batch:  210, Loss:0.606933, AvgAcc:84.0104%\n",
      "[2022-05-10 12:14:53]    Batch:  220, Loss:0.348293, AvgAcc:84.1016%\n",
      "[2022-05-10 12:15:11]    Batch:  230, Loss:0.322958, AvgAcc:84.1372%\n",
      "[2022-05-10 12:15:27]    Batch:  240, Loss:0.436997, AvgAcc:84.1699%\n",
      "[2022-05-10 12:15:44]    Batch:  250, Loss:0.485789, AvgAcc:84.1844%\n",
      "[2022-05-10 12:16:00]    Batch:  260, Loss:0.497975, AvgAcc:84.0925%\n",
      "[2022-05-10 12:16:16]    Batch:  270, Loss:0.499124, AvgAcc:84.0451%\n",
      "[2022-05-10 12:16:32]    Batch:  280, Loss:0.409176, AvgAcc:84.0597%\n",
      "[2022-05-10 12:16:48]    Batch:  290, Loss:0.496309, AvgAcc:83.9925%\n",
      "[2022-05-10 12:17:05]    Batch:  300, Loss:0.498738, AvgAcc:83.9974%\n",
      "[2022-05-10 12:17:22]    Batch:  310, Loss:0.556945, AvgAcc:84.0121%\n",
      "[2022-05-10 12:17:39]    Batch:  320, Loss:0.586284, AvgAcc:84.0430%\n",
      "[2022-05-10 12:17:55]    Batch:  330, Loss:0.358879, AvgAcc:84.0601%\n",
      "[2022-05-10 12:18:12]    Batch:  340, Loss:0.578064, AvgAcc:84.0694%\n",
      "[2022-05-10 12:18:29]    Batch:  350, Loss:0.447967, AvgAcc:84.0804%\n",
      "[2022-05-10 12:18:46]    Batch:  360, Loss:0.579208, AvgAcc:84.1059%\n",
      "[2022-05-10 12:19:06]    Batch:  370, Loss:0.321114, AvgAcc:84.1301%\n",
      "[2022-05-10 12:19:26]    Batch:  380, Loss:0.402437, AvgAcc:84.1365%\n",
      "[2022-05-10 12:19:47]    Batch:  390, Loss:0.368882, AvgAcc:84.1546%\n",
      "Epoch: 20, learning rate = 0.1\n",
      "[2022-05-10 12:21:01]    Batch:   10, Loss:0.360576, AvgAcc:86.5625%\n",
      "[2022-05-10 12:21:22]    Batch:   20, Loss:0.338633, AvgAcc:86.3281%\n",
      "[2022-05-10 12:21:41]    Batch:   30, Loss:0.379367, AvgAcc:85.5990%\n",
      "[2022-05-10 12:22:00]    Batch:   40, Loss:0.506127, AvgAcc:85.6641%\n",
      "[2022-05-10 12:22:19]    Batch:   50, Loss:0.370980, AvgAcc:85.2500%\n",
      "[2022-05-10 12:22:38]    Batch:   60, Loss:0.484281, AvgAcc:84.9609%\n",
      "[2022-05-10 12:22:58]    Batch:   70, Loss:0.417359, AvgAcc:84.9888%\n",
      "[2022-05-10 12:23:21]    Batch:   80, Loss:0.455934, AvgAcc:85.0879%\n",
      "[2022-05-10 12:23:44]    Batch:   90, Loss:0.381891, AvgAcc:84.8264%\n",
      "[2022-05-10 12:24:04]    Batch:  100, Loss:0.466210, AvgAcc:84.7891%\n",
      "[2022-05-10 12:24:22]    Batch:  110, Loss:0.390347, AvgAcc:84.6378%\n",
      "[2022-05-10 12:24:41]    Batch:  120, Loss:0.451341, AvgAcc:84.5247%\n",
      "[2022-05-10 12:25:00]    Batch:  130, Loss:0.423390, AvgAcc:84.6334%\n",
      "[2022-05-10 12:25:19]    Batch:  140, Loss:0.367656, AvgAcc:84.7042%\n",
      "[2022-05-10 12:25:39]    Batch:  150, Loss:0.414234, AvgAcc:84.6250%\n",
      "[2022-05-10 12:25:56]    Batch:  160, Loss:0.465774, AvgAcc:84.5752%\n",
      "[2022-05-10 12:26:15]    Batch:  170, Loss:0.531525, AvgAcc:84.5772%\n",
      "[2022-05-10 12:26:36]    Batch:  180, Loss:0.551246, AvgAcc:84.5052%\n",
      "[2022-05-10 12:26:55]    Batch:  190, Loss:0.464038, AvgAcc:84.4942%\n",
      "[2022-05-10 12:27:13]    Batch:  200, Loss:0.466697, AvgAcc:84.5430%\n",
      "[2022-05-10 12:27:30]    Batch:  210, Loss:0.579214, AvgAcc:84.5647%\n",
      "[2022-05-10 12:27:48]    Batch:  220, Loss:0.414497, AvgAcc:84.6058%\n",
      "[2022-05-10 12:28:11]    Batch:  230, Loss:0.245701, AvgAcc:84.6603%\n",
      "[2022-05-10 12:28:35]    Batch:  240, Loss:0.506360, AvgAcc:84.6712%\n",
      "[2022-05-10 12:28:58]    Batch:  250, Loss:0.395320, AvgAcc:84.6188%\n",
      "[2022-05-10 12:29:17]    Batch:  260, Loss:0.479308, AvgAcc:84.5433%\n",
      "[2022-05-10 12:29:35]    Batch:  270, Loss:0.547882, AvgAcc:84.4792%\n",
      "[2022-05-10 12:29:53]    Batch:  280, Loss:0.438896, AvgAcc:84.5340%\n",
      "[2022-05-10 12:30:10]    Batch:  290, Loss:0.468928, AvgAcc:84.5501%\n",
      "[2022-05-10 12:30:27]    Batch:  300, Loss:0.538690, AvgAcc:84.4844%\n",
      "[2022-05-10 12:30:44]    Batch:  310, Loss:0.439286, AvgAcc:84.4783%\n",
      "[2022-05-10 12:31:04]    Batch:  320, Loss:0.534711, AvgAcc:84.4946%\n",
      "[2022-05-10 12:31:24]    Batch:  330, Loss:0.341385, AvgAcc:84.5904%\n",
      "[2022-05-10 12:31:44]    Batch:  340, Loss:0.430149, AvgAcc:84.5772%\n",
      "[2022-05-10 12:32:02]    Batch:  350, Loss:0.417636, AvgAcc:84.6138%\n",
      "[2022-05-10 12:32:18]    Batch:  360, Loss:0.476510, AvgAcc:84.5790%\n",
      "[2022-05-10 12:32:37]    Batch:  370, Loss:0.454793, AvgAcc:84.5756%\n",
      "[2022-05-10 12:32:55]    Batch:  380, Loss:0.339497, AvgAcc:84.6155%\n",
      "[2022-05-10 12:33:13]    Batch:  390, Loss:0.362782, AvgAcc:84.6394%\n",
      "Epoch: 21, learning rate = 0.1\n",
      "[2022-05-10 12:34:14]    Batch:   10, Loss:0.394808, AvgAcc:85.7812%\n",
      "[2022-05-10 12:34:35]    Batch:   20, Loss:0.361613, AvgAcc:85.2344%\n",
      "[2022-05-10 12:34:58]    Batch:   30, Loss:0.366272, AvgAcc:85.1302%\n",
      "[2022-05-10 12:35:19]    Batch:   40, Loss:0.645642, AvgAcc:85.2148%\n",
      "[2022-05-10 12:35:38]    Batch:   50, Loss:0.372738, AvgAcc:85.2656%\n",
      "[2022-05-10 12:35:57]    Batch:   60, Loss:0.381198, AvgAcc:85.2214%\n",
      "[2022-05-10 12:36:16]    Batch:   70, Loss:0.330355, AvgAcc:85.2232%\n",
      "[2022-05-10 12:36:34]    Batch:   80, Loss:0.499129, AvgAcc:85.2344%\n",
      "[2022-05-10 12:36:54]    Batch:   90, Loss:0.432847, AvgAcc:85.1128%\n",
      "[2022-05-10 12:37:16]    Batch:  100, Loss:0.384677, AvgAcc:85.0156%\n",
      "[2022-05-10 12:37:38]    Batch:  110, Loss:0.389992, AvgAcc:85.0497%\n",
      "[2022-05-10 12:37:59]    Batch:  120, Loss:0.450683, AvgAcc:85.0130%\n",
      "[2022-05-10 12:38:18]    Batch:  130, Loss:0.420443, AvgAcc:85.0721%\n",
      "[2022-05-10 12:38:36]    Batch:  140, Loss:0.346461, AvgAcc:85.0949%\n",
      "[2022-05-10 12:38:56]    Batch:  150, Loss:0.423698, AvgAcc:85.1094%\n",
      "[2022-05-10 12:39:15]    Batch:  160, Loss:0.386861, AvgAcc:85.1318%\n",
      "[2022-05-10 12:39:34]    Batch:  170, Loss:0.459642, AvgAcc:85.1057%\n",
      "[2022-05-10 12:39:53]    Batch:  180, Loss:0.523190, AvgAcc:85.0694%\n",
      "[2022-05-10 12:40:12]    Batch:  190, Loss:0.322373, AvgAcc:85.0082%\n",
      "[2022-05-10 12:40:30]    Batch:  200, Loss:0.495797, AvgAcc:84.9727%\n",
      "[2022-05-10 12:40:48]    Batch:  210, Loss:0.595404, AvgAcc:85.0112%\n",
      "[2022-05-10 12:41:08]    Batch:  220, Loss:0.361793, AvgAcc:85.0426%\n",
      "[2022-05-10 12:41:28]    Batch:  230, Loss:0.359081, AvgAcc:85.0204%\n",
      "[2022-05-10 12:41:46]    Batch:  240, Loss:0.463881, AvgAcc:85.0944%\n",
      "[2022-05-10 12:42:03]    Batch:  250, Loss:0.509350, AvgAcc:85.0219%\n",
      "[2022-05-10 12:42:19]    Batch:  260, Loss:0.435746, AvgAcc:84.9369%\n",
      "[2022-05-10 12:42:35]    Batch:  270, Loss:0.379511, AvgAcc:84.9045%\n",
      "[2022-05-10 12:42:53]    Batch:  280, Loss:0.371468, AvgAcc:84.9526%\n",
      "[2022-05-10 12:43:12]    Batch:  290, Loss:0.438127, AvgAcc:84.9515%\n",
      "[2022-05-10 12:43:31]    Batch:  300, Loss:0.482519, AvgAcc:84.9062%\n",
      "[2022-05-10 12:43:48]    Batch:  310, Loss:0.536647, AvgAcc:84.9420%\n",
      "[2022-05-10 12:44:05]    Batch:  320, Loss:0.470485, AvgAcc:84.9438%\n",
      "[2022-05-10 12:44:21]    Batch:  330, Loss:0.263831, AvgAcc:84.9550%\n",
      "[2022-05-10 12:44:38]    Batch:  340, Loss:0.446620, AvgAcc:84.9770%\n",
      "[2022-05-10 12:44:55]    Batch:  350, Loss:0.529616, AvgAcc:85.0000%\n",
      "[2022-05-10 12:45:12]    Batch:  360, Loss:0.391047, AvgAcc:85.0000%\n",
      "[2022-05-10 12:45:31]    Batch:  370, Loss:0.278510, AvgAcc:84.9979%\n",
      "[2022-05-10 12:45:51]    Batch:  380, Loss:0.370318, AvgAcc:85.0247%\n",
      "[2022-05-10 12:46:11]    Batch:  390, Loss:0.339296, AvgAcc:85.0461%\n",
      "Epoch: 22, learning rate = 0.1\n",
      "[2022-05-10 12:47:15]    Batch:   10, Loss:0.335097, AvgAcc:85.7812%\n",
      "[2022-05-10 12:47:32]    Batch:   20, Loss:0.345835, AvgAcc:86.0938%\n",
      "[2022-05-10 12:47:49]    Batch:   30, Loss:0.344083, AvgAcc:86.2760%\n",
      "[2022-05-10 12:48:06]    Batch:   40, Loss:0.577875, AvgAcc:86.2305%\n",
      "[2022-05-10 12:48:24]    Batch:   50, Loss:0.387880, AvgAcc:86.1719%\n",
      "[2022-05-10 12:48:43]    Batch:   60, Loss:0.492765, AvgAcc:85.8464%\n",
      "[2022-05-10 12:49:04]    Batch:   70, Loss:0.395365, AvgAcc:85.4464%\n",
      "[2022-05-10 12:49:25]    Batch:   80, Loss:0.442429, AvgAcc:85.4980%\n",
      "[2022-05-10 12:49:46]    Batch:   90, Loss:0.307933, AvgAcc:85.4601%\n",
      "[2022-05-10 12:50:08]    Batch:  100, Loss:0.413437, AvgAcc:85.4688%\n",
      "[2022-05-10 12:50:29]    Batch:  110, Loss:0.389460, AvgAcc:85.4048%\n",
      "[2022-05-10 12:50:48]    Batch:  120, Loss:0.462946, AvgAcc:85.5013%\n",
      "[2022-05-10 12:51:07]    Batch:  130, Loss:0.492020, AvgAcc:85.4808%\n",
      "[2022-05-10 12:51:26]    Batch:  140, Loss:0.324272, AvgAcc:85.4576%\n",
      "[2022-05-10 12:51:45]    Batch:  150, Loss:0.455138, AvgAcc:85.4583%\n",
      "[2022-05-10 12:52:03]    Batch:  160, Loss:0.340673, AvgAcc:85.4785%\n",
      "[2022-05-10 12:52:22]    Batch:  170, Loss:0.521709, AvgAcc:85.4458%\n",
      "[2022-05-10 12:52:41]    Batch:  180, Loss:0.522733, AvgAcc:85.4253%\n",
      "[2022-05-10 12:53:00]    Batch:  190, Loss:0.331771, AvgAcc:85.4564%\n",
      "[2022-05-10 12:53:19]    Batch:  200, Loss:0.354907, AvgAcc:85.4688%\n",
      "[2022-05-10 12:53:38]    Batch:  210, Loss:0.581430, AvgAcc:85.5134%\n",
      "[2022-05-10 12:53:57]    Batch:  220, Loss:0.335502, AvgAcc:85.5575%\n",
      "[2022-05-10 12:54:15]    Batch:  230, Loss:0.197170, AvgAcc:85.6250%\n",
      "[2022-05-10 12:54:34]    Batch:  240, Loss:0.307058, AvgAcc:85.6348%\n",
      "[2022-05-10 12:54:53]    Batch:  250, Loss:0.478802, AvgAcc:85.5875%\n",
      "[2022-05-10 12:55:10]    Batch:  260, Loss:0.391251, AvgAcc:85.6010%\n",
      "[2022-05-10 12:55:27]    Batch:  270, Loss:0.361954, AvgAcc:85.5816%\n",
      "[2022-05-10 12:55:44]    Batch:  280, Loss:0.360275, AvgAcc:85.5943%\n",
      "[2022-05-10 12:56:04]    Batch:  290, Loss:0.436485, AvgAcc:85.5792%\n",
      "[2022-05-10 12:56:24]    Batch:  300, Loss:0.502217, AvgAcc:85.5469%\n",
      "[2022-05-10 12:56:44]    Batch:  310, Loss:0.496553, AvgAcc:85.5544%\n",
      "[2022-05-10 12:57:00]    Batch:  320, Loss:0.471045, AvgAcc:85.5542%\n",
      "[2022-05-10 12:57:17]    Batch:  330, Loss:0.240065, AvgAcc:85.5800%\n",
      "[2022-05-10 12:57:34]    Batch:  340, Loss:0.461332, AvgAcc:85.6112%\n",
      "[2022-05-10 12:57:53]    Batch:  350, Loss:0.460759, AvgAcc:85.6094%\n",
      "[2022-05-10 12:58:12]    Batch:  360, Loss:0.478479, AvgAcc:85.6359%\n",
      "[2022-05-10 12:58:34]    Batch:  370, Loss:0.368486, AvgAcc:85.5870%\n",
      "[2022-05-10 12:58:55]    Batch:  380, Loss:0.293274, AvgAcc:85.6476%\n",
      "[2022-05-10 12:59:15]    Batch:  390, Loss:0.329493, AvgAcc:85.6991%\n",
      "Epoch: 23, learning rate = 0.1\n",
      "[2022-05-10 13:00:15]    Batch:   10, Loss:0.329467, AvgAcc:86.4062%\n",
      "[2022-05-10 13:00:35]    Batch:   20, Loss:0.309758, AvgAcc:86.2891%\n",
      "[2022-05-10 13:00:55]    Batch:   30, Loss:0.339222, AvgAcc:86.4323%\n",
      "[2022-05-10 13:01:15]    Batch:   40, Loss:0.641259, AvgAcc:86.5625%\n",
      "[2022-05-10 13:01:33]    Batch:   50, Loss:0.314024, AvgAcc:86.6562%\n",
      "[2022-05-10 13:01:50]    Batch:   60, Loss:0.368489, AvgAcc:86.5495%\n",
      "[2022-05-10 13:02:07]    Batch:   70, Loss:0.314372, AvgAcc:86.3728%\n",
      "[2022-05-10 13:02:23]    Batch:   80, Loss:0.416375, AvgAcc:86.3281%\n",
      "[2022-05-10 13:02:41]    Batch:   90, Loss:0.424211, AvgAcc:86.2240%\n",
      "[2022-05-10 13:02:58]    Batch:  100, Loss:0.411195, AvgAcc:86.2188%\n",
      "[2022-05-10 13:03:14]    Batch:  110, Loss:0.380987, AvgAcc:86.1293%\n",
      "[2022-05-10 13:03:31]    Batch:  120, Loss:0.431159, AvgAcc:86.1003%\n",
      "[2022-05-10 13:03:48]    Batch:  130, Loss:0.405378, AvgAcc:86.1538%\n",
      "[2022-05-10 13:04:06]    Batch:  140, Loss:0.364503, AvgAcc:86.1551%\n",
      "[2022-05-10 13:04:27]    Batch:  150, Loss:0.510919, AvgAcc:86.2292%\n",
      "[2022-05-10 13:04:49]    Batch:  160, Loss:0.346973, AvgAcc:86.2451%\n",
      "[2022-05-10 13:05:11]    Batch:  170, Loss:0.415901, AvgAcc:86.1305%\n",
      "[2022-05-10 13:05:30]    Batch:  180, Loss:0.411969, AvgAcc:86.0938%\n",
      "[2022-05-10 13:05:49]    Batch:  190, Loss:0.337218, AvgAcc:85.9951%\n",
      "[2022-05-10 13:06:08]    Batch:  200, Loss:0.484263, AvgAcc:85.9844%\n",
      "[2022-05-10 13:06:27]    Batch:  210, Loss:0.585078, AvgAcc:86.0417%\n",
      "[2022-05-10 13:06:45]    Batch:  220, Loss:0.299081, AvgAcc:86.0227%\n",
      "[2022-05-10 13:07:04]    Batch:  230, Loss:0.296540, AvgAcc:86.0632%\n",
      "[2022-05-10 13:07:23]    Batch:  240, Loss:0.343256, AvgAcc:86.0840%\n",
      "[2022-05-10 13:07:41]    Batch:  250, Loss:0.476721, AvgAcc:86.0875%\n",
      "[2022-05-10 13:07:58]    Batch:  260, Loss:0.486836, AvgAcc:86.0337%\n",
      "[2022-05-10 13:08:16]    Batch:  270, Loss:0.440406, AvgAcc:86.0041%\n",
      "[2022-05-10 13:08:35]    Batch:  280, Loss:0.333220, AvgAcc:86.0463%\n",
      "[2022-05-10 13:08:55]    Batch:  290, Loss:0.378848, AvgAcc:86.0399%\n",
      "[2022-05-10 13:09:13]    Batch:  300, Loss:0.516892, AvgAcc:85.9792%\n",
      "[2022-05-10 13:09:32]    Batch:  310, Loss:0.514277, AvgAcc:86.0081%\n",
      "[2022-05-10 13:09:53]    Batch:  320, Loss:0.454699, AvgAcc:85.9839%\n",
      "[2022-05-10 13:10:15]    Batch:  330, Loss:0.315450, AvgAcc:86.0275%\n",
      "[2022-05-10 13:10:35]    Batch:  340, Loss:0.394263, AvgAcc:86.0179%\n",
      "[2022-05-10 13:10:57]    Batch:  350, Loss:0.497567, AvgAcc:86.0134%\n",
      "[2022-05-10 13:11:17]    Batch:  360, Loss:0.430630, AvgAcc:86.0221%\n",
      "[2022-05-10 13:11:37]    Batch:  370, Loss:0.330317, AvgAcc:86.0114%\n",
      "[2022-05-10 13:11:54]    Batch:  380, Loss:0.324197, AvgAcc:86.0074%\n",
      "[2022-05-10 13:12:11]    Batch:  390, Loss:0.287798, AvgAcc:86.0296%\n",
      "Epoch: 24, learning rate = 0.1\n",
      "[2022-05-10 13:13:20]    Batch:   10, Loss:0.278337, AvgAcc:86.4062%\n",
      "[2022-05-10 13:13:41]    Batch:   20, Loss:0.285322, AvgAcc:87.0312%\n",
      "[2022-05-10 13:14:00]    Batch:   30, Loss:0.387856, AvgAcc:87.1875%\n",
      "[2022-05-10 13:14:17]    Batch:   40, Loss:0.521268, AvgAcc:87.1094%\n",
      "[2022-05-10 13:14:34]    Batch:   50, Loss:0.340829, AvgAcc:86.9062%\n",
      "[2022-05-10 13:14:51]    Batch:   60, Loss:0.413145, AvgAcc:86.7839%\n",
      "[2022-05-10 13:15:07]    Batch:   70, Loss:0.358062, AvgAcc:86.7746%\n",
      "[2022-05-10 13:15:26]    Batch:   80, Loss:0.447806, AvgAcc:86.6406%\n",
      "[2022-05-10 13:15:46]    Batch:   90, Loss:0.383730, AvgAcc:86.5365%\n",
      "[2022-05-10 13:16:06]    Batch:  100, Loss:0.360505, AvgAcc:86.3438%\n",
      "[2022-05-10 13:16:25]    Batch:  110, Loss:0.404316, AvgAcc:86.3068%\n",
      "[2022-05-10 13:16:41]    Batch:  120, Loss:0.464360, AvgAcc:86.2630%\n",
      "[2022-05-10 13:16:58]    Batch:  130, Loss:0.355985, AvgAcc:86.2981%\n",
      "[2022-05-10 13:17:15]    Batch:  140, Loss:0.423658, AvgAcc:86.2891%\n",
      "[2022-05-10 13:17:32]    Batch:  150, Loss:0.460925, AvgAcc:86.2448%\n",
      "[2022-05-10 13:17:49]    Batch:  160, Loss:0.427586, AvgAcc:86.1963%\n",
      "[2022-05-10 13:18:05]    Batch:  170, Loss:0.399058, AvgAcc:86.1075%\n",
      "[2022-05-10 13:18:22]    Batch:  180, Loss:0.479928, AvgAcc:86.0938%\n",
      "[2022-05-10 13:18:38]    Batch:  190, Loss:0.353913, AvgAcc:86.0362%\n",
      "[2022-05-10 13:18:56]    Batch:  200, Loss:0.397571, AvgAcc:86.1211%\n",
      "[2022-05-10 13:19:13]    Batch:  210, Loss:0.532442, AvgAcc:86.1533%\n",
      "[2022-05-10 13:19:31]    Batch:  220, Loss:0.243702, AvgAcc:86.2820%\n",
      "[2022-05-10 13:19:50]    Batch:  230, Loss:0.278729, AvgAcc:86.3010%\n",
      "[2022-05-10 13:20:09]    Batch:  240, Loss:0.401772, AvgAcc:86.3932%\n",
      "[2022-05-10 13:20:30]    Batch:  250, Loss:0.482789, AvgAcc:86.3031%\n",
      "[2022-05-10 13:20:51]    Batch:  260, Loss:0.390089, AvgAcc:86.2200%\n",
      "[2022-05-10 13:21:12]    Batch:  270, Loss:0.405619, AvgAcc:86.1921%\n",
      "[2022-05-10 13:21:31]    Batch:  280, Loss:0.306631, AvgAcc:86.2333%\n",
      "[2022-05-10 13:21:50]    Batch:  290, Loss:0.415810, AvgAcc:86.2015%\n",
      "[2022-05-10 13:22:09]    Batch:  300, Loss:0.303518, AvgAcc:86.1979%\n",
      "[2022-05-10 13:22:28]    Batch:  310, Loss:0.383217, AvgAcc:86.2172%\n",
      "[2022-05-10 13:22:48]    Batch:  320, Loss:0.460270, AvgAcc:86.2231%\n",
      "[2022-05-10 13:23:07]    Batch:  330, Loss:0.346308, AvgAcc:86.2453%\n",
      "[2022-05-10 13:23:26]    Batch:  340, Loss:0.484262, AvgAcc:86.2201%\n",
      "[2022-05-10 13:23:45]    Batch:  350, Loss:0.525349, AvgAcc:86.2679%\n",
      "[2022-05-10 13:24:04]    Batch:  360, Loss:0.353194, AvgAcc:86.2977%\n",
      "[2022-05-10 13:24:25]    Batch:  370, Loss:0.356786, AvgAcc:86.2901%\n",
      "[2022-05-10 13:24:47]    Batch:  380, Loss:0.253924, AvgAcc:86.2706%\n",
      "[2022-05-10 13:25:10]    Batch:  390, Loss:0.346679, AvgAcc:86.3121%\n",
      "Epoch: 25, learning rate = 0.1\n",
      "[2022-05-10 13:26:19]    Batch:   10, Loss:0.320240, AvgAcc:87.8125%\n",
      "[2022-05-10 13:26:39]    Batch:   20, Loss:0.296113, AvgAcc:87.3828%\n",
      "[2022-05-10 13:26:56]    Batch:   30, Loss:0.262471, AvgAcc:87.6302%\n",
      "[2022-05-10 13:27:13]    Batch:   40, Loss:0.511912, AvgAcc:87.4023%\n",
      "[2022-05-10 13:27:30]    Batch:   50, Loss:0.319313, AvgAcc:87.4219%\n",
      "[2022-05-10 13:27:49]    Batch:   60, Loss:0.398109, AvgAcc:87.2526%\n",
      "[2022-05-10 13:28:08]    Batch:   70, Loss:0.387419, AvgAcc:87.2656%\n",
      "[2022-05-10 13:28:27]    Batch:   80, Loss:0.418920, AvgAcc:87.0312%\n",
      "[2022-05-10 13:28:47]    Batch:   90, Loss:0.352948, AvgAcc:86.8403%\n",
      "[2022-05-10 13:29:11]    Batch:  100, Loss:0.309289, AvgAcc:86.8359%\n",
      "[2022-05-10 13:29:34]    Batch:  110, Loss:0.356369, AvgAcc:86.8608%\n",
      "[2022-05-10 13:29:56]    Batch:  120, Loss:0.497500, AvgAcc:86.7318%\n",
      "[2022-05-10 13:30:13]    Batch:  130, Loss:0.437187, AvgAcc:86.7548%\n",
      "[2022-05-10 13:30:32]    Batch:  140, Loss:0.355333, AvgAcc:86.7746%\n",
      "[2022-05-10 13:30:51]    Batch:  150, Loss:0.374112, AvgAcc:86.7656%\n",
      "[2022-05-10 13:31:11]    Batch:  160, Loss:0.357274, AvgAcc:86.7773%\n",
      "[2022-05-10 13:31:30]    Batch:  170, Loss:0.362404, AvgAcc:86.7463%\n",
      "[2022-05-10 13:31:47]    Batch:  180, Loss:0.357344, AvgAcc:86.7708%\n",
      "[2022-05-10 13:32:04]    Batch:  190, Loss:0.361138, AvgAcc:86.7229%\n",
      "[2022-05-10 13:32:21]    Batch:  200, Loss:0.371316, AvgAcc:86.7500%\n",
      "[2022-05-10 13:32:38]    Batch:  210, Loss:0.570442, AvgAcc:86.8080%\n",
      "[2022-05-10 13:32:55]    Batch:  220, Loss:0.340536, AvgAcc:86.8466%\n",
      "[2022-05-10 13:33:11]    Batch:  230, Loss:0.200578, AvgAcc:86.8818%\n",
      "[2022-05-10 13:33:27]    Batch:  240, Loss:0.474901, AvgAcc:86.9108%\n",
      "[2022-05-10 13:33:43]    Batch:  250, Loss:0.494671, AvgAcc:86.8344%\n",
      "[2022-05-10 13:33:59]    Batch:  260, Loss:0.385021, AvgAcc:86.7879%\n",
      "[2022-05-10 13:34:17]    Batch:  270, Loss:0.519555, AvgAcc:86.7940%\n",
      "[2022-05-10 13:34:35]    Batch:  280, Loss:0.371758, AvgAcc:86.8248%\n",
      "[2022-05-10 13:34:54]    Batch:  290, Loss:0.393190, AvgAcc:86.8023%\n",
      "[2022-05-10 13:35:12]    Batch:  300, Loss:0.461109, AvgAcc:86.7656%\n",
      "[2022-05-10 13:35:31]    Batch:  310, Loss:0.353687, AvgAcc:86.8145%\n",
      "[2022-05-10 13:35:50]    Batch:  320, Loss:0.489379, AvgAcc:86.7847%\n",
      "[2022-05-10 13:36:09]    Batch:  330, Loss:0.292621, AvgAcc:86.8300%\n",
      "[2022-05-10 13:36:28]    Batch:  340, Loss:0.372964, AvgAcc:86.7946%\n",
      "[2022-05-10 13:36:46]    Batch:  350, Loss:0.407967, AvgAcc:86.7991%\n",
      "[2022-05-10 13:37:05]    Batch:  360, Loss:0.388364, AvgAcc:86.8316%\n",
      "[2022-05-10 13:37:23]    Batch:  370, Loss:0.334392, AvgAcc:86.8222%\n",
      "[2022-05-10 13:37:45]    Batch:  380, Loss:0.245041, AvgAcc:86.8812%\n",
      "[2022-05-10 13:38:07]    Batch:  390, Loss:0.335761, AvgAcc:86.8950%\n",
      "Epoch: 26, learning rate = 0.1\n",
      "[2022-05-10 13:39:18]    Batch:   10, Loss:0.325117, AvgAcc:87.7344%\n",
      "[2022-05-10 13:39:37]    Batch:   20, Loss:0.370197, AvgAcc:87.2656%\n",
      "[2022-05-10 13:39:56]    Batch:   30, Loss:0.271911, AvgAcc:87.9167%\n",
      "[2022-05-10 13:40:15]    Batch:   40, Loss:0.508606, AvgAcc:87.9102%\n",
      "[2022-05-10 13:40:36]    Batch:   50, Loss:0.291286, AvgAcc:88.0625%\n",
      "[2022-05-10 13:40:58]    Batch:   60, Loss:0.339366, AvgAcc:87.8776%\n",
      "[2022-05-10 13:41:22]    Batch:   70, Loss:0.267265, AvgAcc:87.9018%\n",
      "[2022-05-10 13:41:42]    Batch:   80, Loss:0.449787, AvgAcc:87.7832%\n",
      "[2022-05-10 13:42:00]    Batch:   90, Loss:0.453767, AvgAcc:87.5434%\n",
      "[2022-05-10 13:42:17]    Batch:  100, Loss:0.301710, AvgAcc:87.3594%\n",
      "[2022-05-10 13:42:33]    Batch:  110, Loss:0.352798, AvgAcc:87.3864%\n",
      "[2022-05-10 13:42:53]    Batch:  120, Loss:0.388217, AvgAcc:87.3438%\n",
      "[2022-05-10 13:43:15]    Batch:  130, Loss:0.469234, AvgAcc:87.3678%\n",
      "[2022-05-10 13:43:37]    Batch:  140, Loss:0.364352, AvgAcc:87.3103%\n",
      "[2022-05-10 13:43:57]    Batch:  150, Loss:0.443106, AvgAcc:87.1927%\n",
      "[2022-05-10 13:44:14]    Batch:  160, Loss:0.422706, AvgAcc:87.1973%\n",
      "[2022-05-10 13:44:31]    Batch:  170, Loss:0.428228, AvgAcc:87.1232%\n",
      "[2022-05-10 13:44:48]    Batch:  180, Loss:0.341720, AvgAcc:87.1701%\n",
      "[2022-05-10 13:45:05]    Batch:  190, Loss:0.318268, AvgAcc:87.1382%\n",
      "[2022-05-10 13:45:22]    Batch:  200, Loss:0.466499, AvgAcc:87.0977%\n",
      "[2022-05-10 13:45:42]    Batch:  210, Loss:0.685708, AvgAcc:87.0536%\n",
      "[2022-05-10 13:46:02]    Batch:  220, Loss:0.220358, AvgAcc:87.1236%\n",
      "[2022-05-10 13:46:23]    Batch:  230, Loss:0.241284, AvgAcc:87.1332%\n",
      "[2022-05-10 13:46:48]    Batch:  240, Loss:0.410374, AvgAcc:87.1647%\n",
      "[2022-05-10 13:47:39]    Batch:  250, Loss:0.409134, AvgAcc:87.1219%\n",
      "[2022-05-10 13:48:30]    Batch:  260, Loss:0.382978, AvgAcc:87.0523%\n",
      "[2022-05-10 13:49:19]    Batch:  270, Loss:0.333382, AvgAcc:87.0544%\n",
      "[2022-05-10 13:50:09]    Batch:  280, Loss:0.320183, AvgAcc:87.0619%\n",
      "[2022-05-10 13:51:01]    Batch:  290, Loss:0.397943, AvgAcc:87.0232%\n",
      "[2022-05-10 13:51:54]    Batch:  300, Loss:0.411617, AvgAcc:87.0000%\n",
      "[2022-05-10 13:52:45]    Batch:  310, Loss:0.439535, AvgAcc:87.0010%\n",
      "[2022-05-10 13:53:42]    Batch:  320, Loss:0.500172, AvgAcc:86.9849%\n",
      "[2022-05-10 13:54:36]    Batch:  330, Loss:0.263104, AvgAcc:86.9934%\n",
      "[2022-05-10 13:55:28]    Batch:  340, Loss:0.310397, AvgAcc:87.0060%\n",
      "[2022-05-10 13:56:16]    Batch:  350, Loss:0.408439, AvgAcc:87.0491%\n",
      "[2022-05-10 13:57:05]    Batch:  360, Loss:0.355029, AvgAcc:87.0573%\n",
      "[2022-05-10 13:57:53]    Batch:  370, Loss:0.260854, AvgAcc:87.0629%\n",
      "[2022-05-10 13:58:44]    Batch:  380, Loss:0.293661, AvgAcc:87.0456%\n",
      "[2022-05-10 13:59:33]    Batch:  390, Loss:0.330584, AvgAcc:87.0713%\n",
      "Epoch: 27, learning rate = 0.1\n",
      "[2022-05-10 14:02:25]    Batch:   10, Loss:0.236861, AvgAcc:88.2812%\n",
      "[2022-05-10 14:03:23]    Batch:   20, Loss:0.243563, AvgAcc:87.9688%\n",
      "[2022-05-10 14:04:13]    Batch:   30, Loss:0.297528, AvgAcc:88.2292%\n",
      "[2022-05-10 14:05:04]    Batch:   40, Loss:0.513126, AvgAcc:88.3398%\n",
      "[2022-05-10 14:05:57]    Batch:   50, Loss:0.292015, AvgAcc:88.2656%\n",
      "[2022-05-10 14:06:53]    Batch:   60, Loss:0.319215, AvgAcc:88.2161%\n",
      "[2022-05-10 14:07:47]    Batch:   70, Loss:0.365220, AvgAcc:88.2366%\n",
      "[2022-05-10 14:08:40]    Batch:   80, Loss:0.514293, AvgAcc:88.1348%\n",
      "[2022-05-10 14:09:33]    Batch:   90, Loss:0.338470, AvgAcc:88.0556%\n",
      "[2022-05-10 14:10:29]    Batch:  100, Loss:0.424063, AvgAcc:87.9375%\n",
      "[2022-05-10 14:11:27]    Batch:  110, Loss:0.288075, AvgAcc:87.7841%\n",
      "[2022-05-10 14:12:15]    Batch:  120, Loss:0.398422, AvgAcc:87.7083%\n",
      "[2022-05-10 14:13:13]    Batch:  130, Loss:0.389752, AvgAcc:87.7163%\n",
      "[2022-05-10 14:14:06]    Batch:  140, Loss:0.320321, AvgAcc:87.7009%\n",
      "[2022-05-10 14:15:00]    Batch:  150, Loss:0.410936, AvgAcc:87.5625%\n",
      "[2022-05-10 14:15:58]    Batch:  160, Loss:0.490058, AvgAcc:87.5049%\n",
      "[2022-05-10 14:16:47]    Batch:  170, Loss:0.380036, AvgAcc:87.4678%\n",
      "[2022-05-10 14:17:38]    Batch:  180, Loss:0.456563, AvgAcc:87.4175%\n",
      "[2022-05-10 14:18:39]    Batch:  190, Loss:0.291387, AvgAcc:87.3643%\n",
      "[2022-05-10 14:19:33]    Batch:  200, Loss:0.420392, AvgAcc:87.3633%\n",
      "[2022-05-10 14:20:25]    Batch:  210, Loss:0.469261, AvgAcc:87.4330%\n",
      "[2022-05-10 14:21:19]    Batch:  220, Loss:0.277075, AvgAcc:87.4290%\n",
      "[2022-05-10 14:22:15]    Batch:  230, Loss:0.272494, AvgAcc:87.4796%\n",
      "[2022-05-10 14:23:08]    Batch:  240, Loss:0.333129, AvgAcc:87.4772%\n",
      "[2022-05-10 14:24:02]    Batch:  250, Loss:0.411750, AvgAcc:87.4094%\n",
      "[2022-05-10 14:24:59]    Batch:  260, Loss:0.441681, AvgAcc:87.3768%\n",
      "[2022-05-10 14:25:54]    Batch:  270, Loss:0.348164, AvgAcc:87.2888%\n",
      "[2022-05-10 14:26:43]    Batch:  280, Loss:0.300060, AvgAcc:87.2991%\n",
      "[2022-05-10 14:27:36]    Batch:  290, Loss:0.421697, AvgAcc:87.2629%\n",
      "[2022-05-10 14:28:31]    Batch:  300, Loss:0.350790, AvgAcc:87.2812%\n",
      "[2022-05-10 14:29:27]    Batch:  310, Loss:0.386142, AvgAcc:87.2984%\n",
      "[2022-05-10 14:30:24]    Batch:  320, Loss:0.461079, AvgAcc:87.2656%\n",
      "[2022-05-10 14:31:19]    Batch:  330, Loss:0.285974, AvgAcc:87.3224%\n",
      "[2022-05-10 14:32:06]    Batch:  340, Loss:0.351042, AvgAcc:87.3552%\n",
      "[2022-05-10 14:33:00]    Batch:  350, Loss:0.396926, AvgAcc:87.3929%\n",
      "[2022-05-10 14:33:56]    Batch:  360, Loss:0.338894, AvgAcc:87.3828%\n",
      "[2022-05-10 14:34:47]    Batch:  370, Loss:0.290325, AvgAcc:87.3986%\n",
      "[2022-05-10 14:35:39]    Batch:  380, Loss:0.226985, AvgAcc:87.4322%\n",
      "[2022-05-10 14:36:30]    Batch:  390, Loss:0.258518, AvgAcc:87.4800%\n",
      "Epoch: 28, learning rate = 0.1\n",
      "[2022-05-10 14:39:40]    Batch:   10, Loss:0.289452, AvgAcc:87.4219%\n",
      "[2022-05-10 14:40:33]    Batch:   20, Loss:0.276121, AvgAcc:87.6172%\n",
      "[2022-05-10 14:41:26]    Batch:   30, Loss:0.325264, AvgAcc:87.5000%\n",
      "[2022-05-10 14:42:14]    Batch:   40, Loss:0.581791, AvgAcc:87.3633%\n",
      "[2022-05-10 14:43:10]    Batch:   50, Loss:0.272213, AvgAcc:87.2031%\n",
      "[2022-05-10 14:44:04]    Batch:   60, Loss:0.370010, AvgAcc:87.3698%\n",
      "[2022-05-10 14:44:52]    Batch:   70, Loss:0.260918, AvgAcc:87.4665%\n",
      "[2022-05-10 14:45:45]    Batch:   80, Loss:0.375912, AvgAcc:87.5488%\n",
      "[2022-05-10 14:46:37]    Batch:   90, Loss:0.305186, AvgAcc:87.4826%\n",
      "[2022-05-10 14:47:15]    Batch:  100, Loss:0.273654, AvgAcc:87.4922%\n",
      "[2022-05-10 14:47:33]    Batch:  110, Loss:0.368872, AvgAcc:87.4290%\n",
      "[2022-05-10 14:47:50]    Batch:  120, Loss:0.465463, AvgAcc:87.5065%\n",
      "[2022-05-10 14:48:06]    Batch:  130, Loss:0.387065, AvgAcc:87.4639%\n",
      "[2022-05-10 14:48:23]    Batch:  140, Loss:0.295181, AvgAcc:87.4219%\n",
      "[2022-05-10 14:48:40]    Batch:  150, Loss:0.421728, AvgAcc:87.4583%\n",
      "[2022-05-10 14:48:58]    Batch:  160, Loss:0.389553, AvgAcc:87.4072%\n",
      "[2022-05-10 14:49:15]    Batch:  170, Loss:0.402322, AvgAcc:87.3254%\n",
      "[2022-05-10 14:49:34]    Batch:  180, Loss:0.360215, AvgAcc:87.2569%\n",
      "[2022-05-10 14:49:53]    Batch:  190, Loss:0.394241, AvgAcc:87.1957%\n",
      "[2022-05-10 14:50:12]    Batch:  200, Loss:0.442832, AvgAcc:87.1445%\n",
      "[2022-05-10 14:50:32]    Batch:  210, Loss:0.468481, AvgAcc:87.2582%\n",
      "[2022-05-10 14:50:54]    Batch:  220, Loss:0.276969, AvgAcc:87.3189%\n",
      "[2022-05-10 14:51:15]    Batch:  230, Loss:0.170764, AvgAcc:87.3438%\n",
      "[2022-05-10 14:51:35]    Batch:  240, Loss:0.380848, AvgAcc:87.3796%\n",
      "[2022-05-10 14:51:53]    Batch:  250, Loss:0.423324, AvgAcc:87.3469%\n",
      "[2022-05-10 14:52:11]    Batch:  260, Loss:0.315311, AvgAcc:87.3468%\n",
      "[2022-05-10 14:52:30]    Batch:  270, Loss:0.402839, AvgAcc:87.3322%\n",
      "[2022-05-10 14:52:49]    Batch:  280, Loss:0.353681, AvgAcc:87.3661%\n",
      "[2022-05-10 14:53:08]    Batch:  290, Loss:0.416736, AvgAcc:87.2980%\n",
      "[2022-05-10 14:53:27]    Batch:  300, Loss:0.345260, AvgAcc:87.3203%\n",
      "[2022-05-10 14:53:46]    Batch:  310, Loss:0.396015, AvgAcc:87.3564%\n",
      "[2022-05-10 14:54:06]    Batch:  320, Loss:0.388329, AvgAcc:87.3608%\n",
      "[2022-05-10 14:54:24]    Batch:  330, Loss:0.264235, AvgAcc:87.4100%\n",
      "[2022-05-10 14:54:43]    Batch:  340, Loss:0.450222, AvgAcc:87.4540%\n",
      "[2022-05-10 14:55:02]    Batch:  350, Loss:0.389104, AvgAcc:87.4665%\n",
      "[2022-05-10 14:55:21]    Batch:  360, Loss:0.330947, AvgAcc:87.4805%\n",
      "[2022-05-10 14:55:38]    Batch:  370, Loss:0.315233, AvgAcc:87.4641%\n",
      "[2022-05-10 14:56:00]    Batch:  380, Loss:0.225453, AvgAcc:87.5021%\n",
      "[2022-05-10 14:56:23]    Batch:  390, Loss:0.263747, AvgAcc:87.5280%\n",
      "Epoch: 29, learning rate = 0.1\n",
      "[2022-05-10 14:57:28]    Batch:   10, Loss:0.306688, AvgAcc:88.0469%\n",
      "[2022-05-10 14:57:46]    Batch:   20, Loss:0.232599, AvgAcc:87.6953%\n",
      "[2022-05-10 14:58:05]    Batch:   30, Loss:0.216651, AvgAcc:88.0729%\n",
      "[2022-05-10 14:58:24]    Batch:   40, Loss:0.677208, AvgAcc:87.7734%\n",
      "[2022-05-10 14:58:42]    Batch:   50, Loss:0.279840, AvgAcc:87.7969%\n",
      "[2022-05-10 14:59:00]    Batch:   60, Loss:0.366635, AvgAcc:87.8646%\n",
      "[2022-05-10 14:59:19]    Batch:   70, Loss:0.339556, AvgAcc:87.7902%\n",
      "[2022-05-10 14:59:39]    Batch:   80, Loss:0.442197, AvgAcc:87.6367%\n",
      "[2022-05-10 15:00:00]    Batch:   90, Loss:0.362571, AvgAcc:87.6042%\n",
      "[2022-05-10 15:00:19]    Batch:  100, Loss:0.318399, AvgAcc:87.7188%\n",
      "[2022-05-10 15:00:39]    Batch:  110, Loss:0.310416, AvgAcc:87.7344%\n",
      "[2022-05-10 15:00:59]    Batch:  120, Loss:0.480424, AvgAcc:87.6497%\n",
      "[2022-05-10 15:01:19]    Batch:  130, Loss:0.384842, AvgAcc:87.7524%\n",
      "[2022-05-10 15:01:37]    Batch:  140, Loss:0.304567, AvgAcc:87.6618%\n",
      "[2022-05-10 15:01:54]    Batch:  150, Loss:0.421515, AvgAcc:87.6615%\n",
      "[2022-05-10 15:02:11]    Batch:  160, Loss:0.518846, AvgAcc:87.6562%\n",
      "[2022-05-10 15:02:28]    Batch:  170, Loss:0.390997, AvgAcc:87.6379%\n",
      "[2022-05-10 15:02:44]    Batch:  180, Loss:0.363706, AvgAcc:87.6128%\n",
      "[2022-05-10 15:03:01]    Batch:  190, Loss:0.358478, AvgAcc:87.6645%\n",
      "[2022-05-10 15:03:18]    Batch:  200, Loss:0.385245, AvgAcc:87.6641%\n",
      "[2022-05-10 15:03:35]    Batch:  210, Loss:0.424182, AvgAcc:87.6823%\n",
      "[2022-05-10 15:03:51]    Batch:  220, Loss:0.270349, AvgAcc:87.7521%\n",
      "[2022-05-10 15:04:07]    Batch:  230, Loss:0.201430, AvgAcc:87.8023%\n",
      "[2022-05-10 15:04:24]    Batch:  240, Loss:0.380665, AvgAcc:87.8255%\n",
      "[2022-05-10 15:04:43]    Batch:  250, Loss:0.419344, AvgAcc:87.8187%\n",
      "[2022-05-10 15:05:02]    Batch:  260, Loss:0.333786, AvgAcc:87.7825%\n",
      "[2022-05-10 15:05:21]    Batch:  270, Loss:0.442229, AvgAcc:87.7402%\n",
      "[2022-05-10 15:05:41]    Batch:  280, Loss:0.349877, AvgAcc:87.8069%\n",
      "[2022-05-10 15:06:03]    Batch:  290, Loss:0.335193, AvgAcc:87.7829%\n",
      "[2022-05-10 15:06:26]    Batch:  300, Loss:0.487138, AvgAcc:87.7630%\n",
      "[2022-05-10 15:06:47]    Batch:  310, Loss:0.369972, AvgAcc:87.7142%\n",
      "[2022-05-10 15:07:06]    Batch:  320, Loss:0.502300, AvgAcc:87.6709%\n",
      "[2022-05-10 15:07:25]    Batch:  330, Loss:0.237688, AvgAcc:87.6752%\n",
      "[2022-05-10 15:07:44]    Batch:  340, Loss:0.322946, AvgAcc:87.6838%\n",
      "[2022-05-10 15:08:02]    Batch:  350, Loss:0.429768, AvgAcc:87.7277%\n",
      "[2022-05-10 15:08:21]    Batch:  360, Loss:0.432470, AvgAcc:87.7148%\n",
      "[2022-05-10 15:08:40]    Batch:  370, Loss:0.271795, AvgAcc:87.7280%\n",
      "[2022-05-10 15:09:00]    Batch:  380, Loss:0.254265, AvgAcc:87.7118%\n",
      "[2022-05-10 15:09:19]    Batch:  390, Loss:0.273186, AvgAcc:87.7163%\n",
      "Epoch: 30, learning rate = 0.1\n",
      "[2022-05-10 15:10:25]    Batch:   10, Loss:0.281184, AvgAcc:88.5156%\n",
      "[2022-05-10 15:10:43]    Batch:   20, Loss:0.340814, AvgAcc:88.5547%\n",
      "[2022-05-10 15:11:07]    Batch:   30, Loss:0.307573, AvgAcc:88.5417%\n",
      "[2022-05-10 15:11:30]    Batch:   40, Loss:0.465894, AvgAcc:88.6719%\n",
      "[2022-05-10 15:11:50]    Batch:   50, Loss:0.255134, AvgAcc:88.7500%\n",
      "[2022-05-10 15:12:07]    Batch:   60, Loss:0.355601, AvgAcc:88.6068%\n",
      "[2022-05-10 15:12:24]    Batch:   70, Loss:0.337130, AvgAcc:88.7054%\n",
      "[2022-05-10 15:12:42]    Batch:   80, Loss:0.329327, AvgAcc:88.7402%\n",
      "[2022-05-10 15:13:01]    Batch:   90, Loss:0.215564, AvgAcc:88.7587%\n",
      "[2022-05-10 15:13:20]    Batch:  100, Loss:0.225500, AvgAcc:88.7656%\n",
      "[2022-05-10 15:13:39]    Batch:  110, Loss:0.341423, AvgAcc:88.6719%\n",
      "[2022-05-10 15:13:56]    Batch:  120, Loss:0.417264, AvgAcc:88.6719%\n",
      "[2022-05-10 15:14:13]    Batch:  130, Loss:0.309909, AvgAcc:88.7200%\n",
      "[2022-05-10 15:14:30]    Batch:  140, Loss:0.207870, AvgAcc:88.6942%\n",
      "[2022-05-10 15:14:50]    Batch:  150, Loss:0.361764, AvgAcc:88.5625%\n",
      "[2022-05-10 15:15:10]    Batch:  160, Loss:0.357890, AvgAcc:88.5645%\n",
      "[2022-05-10 15:15:31]    Batch:  170, Loss:0.419861, AvgAcc:88.5018%\n",
      "[2022-05-10 15:15:55]    Batch:  180, Loss:0.385827, AvgAcc:88.4245%\n",
      "[2022-05-10 15:16:18]    Batch:  190, Loss:0.207436, AvgAcc:88.4992%\n",
      "[2022-05-10 15:16:39]    Batch:  200, Loss:0.407970, AvgAcc:88.4883%\n",
      "[2022-05-10 15:16:56]    Batch:  210, Loss:0.376195, AvgAcc:88.5826%\n",
      "[2022-05-10 15:17:11]    Batch:  220, Loss:0.233672, AvgAcc:88.6612%\n",
      "[2022-05-10 15:17:27]    Batch:  230, Loss:0.200829, AvgAcc:88.6345%\n",
      "[2022-05-10 15:17:44]    Batch:  240, Loss:0.319174, AvgAcc:88.6686%\n",
      "[2022-05-10 15:18:00]    Batch:  250, Loss:0.356365, AvgAcc:88.6594%\n",
      "[2022-05-10 15:18:17]    Batch:  260, Loss:0.331614, AvgAcc:88.5877%\n",
      "[2022-05-10 15:18:34]    Batch:  270, Loss:0.389001, AvgAcc:88.5851%\n",
      "[2022-05-10 15:18:51]    Batch:  280, Loss:0.343506, AvgAcc:88.5770%\n",
      "[2022-05-10 15:19:09]    Batch:  290, Loss:0.404205, AvgAcc:88.5291%\n",
      "[2022-05-10 15:19:26]    Batch:  300, Loss:0.311662, AvgAcc:88.5286%\n",
      "[2022-05-10 15:19:43]    Batch:  310, Loss:0.358666, AvgAcc:88.5383%\n",
      "[2022-05-10 15:19:59]    Batch:  320, Loss:0.317693, AvgAcc:88.5010%\n",
      "[2022-05-10 15:20:16]    Batch:  330, Loss:0.271403, AvgAcc:88.5085%\n",
      "[2022-05-10 15:20:33]    Batch:  340, Loss:0.359112, AvgAcc:88.4881%\n",
      "[2022-05-10 15:20:50]    Batch:  350, Loss:0.405468, AvgAcc:88.5000%\n",
      "[2022-05-10 15:21:06]    Batch:  360, Loss:0.411810, AvgAcc:88.5091%\n",
      "[2022-05-10 15:21:23]    Batch:  370, Loss:0.293216, AvgAcc:88.5114%\n",
      "[2022-05-10 15:21:41]    Batch:  380, Loss:0.173870, AvgAcc:88.5115%\n",
      "[2022-05-10 15:21:57]    Batch:  390, Loss:0.229658, AvgAcc:88.5196%\n",
      "Epoch: 31, learning rate = 0.1\n",
      "[2022-05-10 15:22:57]    Batch:   10, Loss:0.250927, AvgAcc:89.4531%\n",
      "[2022-05-10 15:23:13]    Batch:   20, Loss:0.282248, AvgAcc:89.3750%\n",
      "[2022-05-10 15:23:30]    Batch:   30, Loss:0.306974, AvgAcc:89.3229%\n",
      "[2022-05-10 15:23:47]    Batch:   40, Loss:0.481613, AvgAcc:89.4727%\n",
      "[2022-05-10 15:24:04]    Batch:   50, Loss:0.239910, AvgAcc:89.1719%\n",
      "[2022-05-10 15:24:22]    Batch:   60, Loss:0.311661, AvgAcc:88.7760%\n",
      "[2022-05-10 15:24:42]    Batch:   70, Loss:0.321885, AvgAcc:88.7612%\n",
      "[2022-05-10 15:25:02]    Batch:   80, Loss:0.321272, AvgAcc:88.7500%\n",
      "[2022-05-10 15:25:21]    Batch:   90, Loss:0.280892, AvgAcc:88.6111%\n",
      "[2022-05-10 15:25:38]    Batch:  100, Loss:0.259831, AvgAcc:88.6406%\n",
      "[2022-05-10 15:25:56]    Batch:  110, Loss:0.278648, AvgAcc:88.5511%\n",
      "[2022-05-10 15:26:16]    Batch:  120, Loss:0.382876, AvgAcc:88.4831%\n",
      "[2022-05-10 15:26:36]    Batch:  130, Loss:0.382536, AvgAcc:88.6058%\n",
      "[2022-05-10 15:26:55]    Batch:  140, Loss:0.316057, AvgAcc:88.6217%\n",
      "[2022-05-10 15:27:12]    Batch:  150, Loss:0.418523, AvgAcc:88.6458%\n",
      "[2022-05-10 15:27:29]    Batch:  160, Loss:0.366105, AvgAcc:88.5938%\n",
      "[2022-05-10 15:27:46]    Batch:  170, Loss:0.322034, AvgAcc:88.5616%\n",
      "[2022-05-10 15:28:05]    Batch:  180, Loss:0.306825, AvgAcc:88.5720%\n",
      "[2022-05-10 15:28:25]    Batch:  190, Loss:0.294685, AvgAcc:88.5403%\n",
      "[2022-05-10 15:28:45]    Batch:  200, Loss:0.378508, AvgAcc:88.4727%\n",
      "[2022-05-10 15:29:03]    Batch:  210, Loss:0.467032, AvgAcc:88.5045%\n",
      "[2022-05-10 15:29:19]    Batch:  220, Loss:0.197568, AvgAcc:88.5334%\n",
      "[2022-05-10 15:29:34]    Batch:  230, Loss:0.184648, AvgAcc:88.5394%\n",
      "[2022-05-10 15:29:51]    Batch:  240, Loss:0.433560, AvgAcc:88.5319%\n",
      "[2022-05-10 15:30:08]    Batch:  250, Loss:0.274393, AvgAcc:88.5250%\n",
      "[2022-05-10 15:30:25]    Batch:  260, Loss:0.320013, AvgAcc:88.4826%\n",
      "[2022-05-10 15:30:42]    Batch:  270, Loss:0.346491, AvgAcc:88.4346%\n",
      "[2022-05-10 15:30:59]    Batch:  280, Loss:0.240321, AvgAcc:88.4710%\n",
      "[2022-05-10 15:31:16]    Batch:  290, Loss:0.299770, AvgAcc:88.4483%\n",
      "[2022-05-10 15:31:33]    Batch:  300, Loss:0.304040, AvgAcc:88.4375%\n",
      "[2022-05-10 15:31:50]    Batch:  310, Loss:0.358324, AvgAcc:88.4753%\n",
      "[2022-05-10 15:32:07]    Batch:  320, Loss:0.363764, AvgAcc:88.4790%\n",
      "[2022-05-10 15:32:24]    Batch:  330, Loss:0.231603, AvgAcc:88.4991%\n",
      "[2022-05-10 15:32:41]    Batch:  340, Loss:0.299769, AvgAcc:88.4903%\n",
      "[2022-05-10 15:32:57]    Batch:  350, Loss:0.341265, AvgAcc:88.5335%\n",
      "[2022-05-10 15:33:14]    Batch:  360, Loss:0.422633, AvgAcc:88.5243%\n",
      "[2022-05-10 15:33:31]    Batch:  370, Loss:0.281417, AvgAcc:88.4840%\n",
      "[2022-05-10 15:33:48]    Batch:  380, Loss:0.172783, AvgAcc:88.4725%\n",
      "[2022-05-10 15:34:05]    Batch:  390, Loss:0.241977, AvgAcc:88.4876%\n",
      "Epoch: 32, learning rate = 0.1\n",
      "[2022-05-10 15:35:05]    Batch:   10, Loss:0.231269, AvgAcc:87.8906%\n",
      "[2022-05-10 15:35:22]    Batch:   20, Loss:0.254558, AvgAcc:88.0469%\n",
      "[2022-05-10 15:35:38]    Batch:   30, Loss:0.266200, AvgAcc:88.3854%\n",
      "[2022-05-10 15:35:58]    Batch:   40, Loss:0.554730, AvgAcc:88.4180%\n",
      "[2022-05-10 15:36:17]    Batch:   50, Loss:0.236523, AvgAcc:88.5469%\n",
      "[2022-05-10 15:36:38]    Batch:   60, Loss:0.391442, AvgAcc:88.6198%\n",
      "[2022-05-10 15:36:59]    Batch:   70, Loss:0.294326, AvgAcc:88.5826%\n",
      "[2022-05-10 15:37:19]    Batch:   80, Loss:0.428334, AvgAcc:88.3008%\n",
      "[2022-05-10 15:37:39]    Batch:   90, Loss:0.328494, AvgAcc:88.1771%\n",
      "[2022-05-10 15:37:55]    Batch:  100, Loss:0.213104, AvgAcc:88.2578%\n",
      "[2022-05-10 15:38:12]    Batch:  110, Loss:0.292078, AvgAcc:88.1818%\n",
      "[2022-05-10 15:38:29]    Batch:  120, Loss:0.374831, AvgAcc:88.2747%\n",
      "[2022-05-10 15:38:46]    Batch:  130, Loss:0.374496, AvgAcc:88.2212%\n",
      "[2022-05-10 15:39:03]    Batch:  140, Loss:0.245286, AvgAcc:88.1864%\n",
      "[2022-05-10 15:39:20]    Batch:  150, Loss:0.410505, AvgAcc:88.1406%\n",
      "[2022-05-10 15:39:37]    Batch:  160, Loss:0.346962, AvgAcc:88.0664%\n",
      "[2022-05-10 15:39:54]    Batch:  170, Loss:0.363352, AvgAcc:88.0607%\n",
      "[2022-05-10 15:40:11]    Batch:  180, Loss:0.239277, AvgAcc:88.0599%\n",
      "[2022-05-10 15:40:28]    Batch:  190, Loss:0.322412, AvgAcc:87.9770%\n",
      "[2022-05-10 15:40:45]    Batch:  200, Loss:0.392514, AvgAcc:88.0039%\n",
      "[2022-05-10 15:41:01]    Batch:  210, Loss:0.425253, AvgAcc:88.0952%\n",
      "[2022-05-10 15:41:16]    Batch:  220, Loss:0.242987, AvgAcc:88.1499%\n",
      "[2022-05-10 15:41:33]    Batch:  230, Loss:0.234014, AvgAcc:88.2201%\n",
      "[2022-05-10 15:41:50]    Batch:  240, Loss:0.300159, AvgAcc:88.3105%\n",
      "[2022-05-10 15:42:07]    Batch:  250, Loss:0.327028, AvgAcc:88.3563%\n",
      "[2022-05-10 15:42:24]    Batch:  260, Loss:0.369350, AvgAcc:88.3023%\n",
      "[2022-05-10 15:42:41]    Batch:  270, Loss:0.395769, AvgAcc:88.2494%\n",
      "[2022-05-10 15:42:58]    Batch:  280, Loss:0.297004, AvgAcc:88.2394%\n",
      "[2022-05-10 15:43:17]    Batch:  290, Loss:0.288463, AvgAcc:88.2139%\n",
      "[2022-05-10 15:43:37]    Batch:  300, Loss:0.377926, AvgAcc:88.2448%\n",
      "[2022-05-10 15:43:57]    Batch:  310, Loss:0.290112, AvgAcc:88.2510%\n",
      "[2022-05-10 15:44:14]    Batch:  320, Loss:0.426305, AvgAcc:88.3105%\n",
      "[2022-05-10 15:44:31]    Batch:  330, Loss:0.261539, AvgAcc:88.3617%\n",
      "[2022-05-10 15:44:48]    Batch:  340, Loss:0.280978, AvgAcc:88.3915%\n",
      "[2022-05-10 15:45:05]    Batch:  350, Loss:0.452614, AvgAcc:88.4263%\n",
      "[2022-05-10 15:45:21]    Batch:  360, Loss:0.383266, AvgAcc:88.4223%\n",
      "[2022-05-10 15:45:38]    Batch:  370, Loss:0.280691, AvgAcc:88.3974%\n",
      "[2022-05-10 15:45:55]    Batch:  380, Loss:0.208615, AvgAcc:88.4025%\n",
      "[2022-05-10 15:46:12]    Batch:  390, Loss:0.343574, AvgAcc:88.4255%\n",
      "Epoch: 33, learning rate = 0.01\n",
      "[2022-05-10 15:47:11]    Batch:   10, Loss:0.364935, AvgAcc:89.2188%\n",
      "[2022-05-10 15:47:28]    Batch:   20, Loss:0.269157, AvgAcc:88.8281%\n",
      "[2022-05-10 15:47:45]    Batch:   30, Loss:0.313929, AvgAcc:89.0625%\n",
      "[2022-05-10 15:48:04]    Batch:   40, Loss:0.492509, AvgAcc:88.8086%\n",
      "[2022-05-10 15:48:24]    Batch:   50, Loss:0.271772, AvgAcc:88.8750%\n",
      "[2022-05-10 15:48:44]    Batch:   60, Loss:0.332950, AvgAcc:88.8281%\n",
      "[2022-05-10 15:49:03]    Batch:   70, Loss:0.297351, AvgAcc:88.7946%\n",
      "[2022-05-10 15:49:20]    Batch:   80, Loss:0.360195, AvgAcc:88.7988%\n",
      "[2022-05-10 15:49:37]    Batch:   90, Loss:0.332673, AvgAcc:88.6372%\n",
      "[2022-05-10 15:49:54]    Batch:  100, Loss:0.277852, AvgAcc:88.6641%\n",
      "[2022-05-10 15:50:11]    Batch:  110, Loss:0.359261, AvgAcc:88.6719%\n",
      "[2022-05-10 15:50:28]    Batch:  120, Loss:0.412913, AvgAcc:88.6979%\n",
      "[2022-05-10 15:50:45]    Batch:  130, Loss:0.395666, AvgAcc:88.6599%\n",
      "[2022-05-10 15:51:02]    Batch:  140, Loss:0.285657, AvgAcc:88.6886%\n",
      "[2022-05-10 15:51:19]    Batch:  150, Loss:0.306907, AvgAcc:88.7188%\n",
      "[2022-05-10 15:51:36]    Batch:  160, Loss:0.346821, AvgAcc:88.7402%\n",
      "[2022-05-10 15:51:55]    Batch:  170, Loss:0.346686, AvgAcc:88.7408%\n",
      "[2022-05-10 15:52:14]    Batch:  180, Loss:0.307855, AvgAcc:88.7023%\n",
      "[2022-05-10 15:52:34]    Batch:  190, Loss:0.333302, AvgAcc:88.7212%\n",
      "[2022-05-10 15:52:52]    Batch:  200, Loss:0.422499, AvgAcc:88.6406%\n",
      "[2022-05-10 15:53:08]    Batch:  210, Loss:0.410717, AvgAcc:88.6905%\n",
      "[2022-05-10 15:53:24]    Batch:  220, Loss:0.330838, AvgAcc:88.7500%\n",
      "[2022-05-10 15:53:40]    Batch:  230, Loss:0.153604, AvgAcc:88.8247%\n",
      "[2022-05-10 15:53:57]    Batch:  240, Loss:0.258865, AvgAcc:88.8770%\n",
      "[2022-05-10 15:54:14]    Batch:  250, Loss:0.376495, AvgAcc:88.8625%\n",
      "[2022-05-10 15:54:30]    Batch:  260, Loss:0.312781, AvgAcc:88.8311%\n",
      "[2022-05-10 15:54:47]    Batch:  270, Loss:0.322362, AvgAcc:88.7876%\n",
      "[2022-05-10 15:55:04]    Batch:  280, Loss:0.259470, AvgAcc:88.8170%\n",
      "[2022-05-10 15:55:21]    Batch:  290, Loss:0.322189, AvgAcc:88.8093%\n",
      "[2022-05-10 15:55:38]    Batch:  300, Loss:0.395400, AvgAcc:88.7526%\n",
      "[2022-05-10 15:55:55]    Batch:  310, Loss:0.354048, AvgAcc:88.7626%\n",
      "[2022-05-10 15:56:12]    Batch:  320, Loss:0.451570, AvgAcc:88.7036%\n",
      "[2022-05-10 15:56:29]    Batch:  330, Loss:0.252197, AvgAcc:88.7405%\n",
      "[2022-05-10 15:56:46]    Batch:  340, Loss:0.353695, AvgAcc:88.7339%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import time\n",
    "import numpy as np\n",
    "import ResNet56\n",
    "import mydatasets\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def loadData():\n",
    "    cifar10path = '.'\n",
    "    data_transforms = transforms.Compose([\n",
    "\n",
    "        transforms.RandomCrop(32, padding=4), #随机裁剪\n",
    "        transforms.RandomHorizontalFlip(), # 翻转图片\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    train_dataset = mydatasets.MyCifar10(cifar10path, True, data_transforms, True)\n",
    "    train_loader = mydatasets.MyDataLoader(train_dataset, 128)\n",
    "    test_dataset = mydatasets.MyCifar10(cifar10path, False, data_transforms)\n",
    "    test_loader = mydatasets.MyDataLoader(test_dataset, 128)\n",
    "    return (train_loader, test_loader)\n",
    "\n",
    "\n",
    "\n",
    "def train(trainLoader, model, lossFunction, optimizer, device,data_new):\n",
    "    model.train()\n",
    "    tloss, totalBatch = 0, 0\n",
    "    correct, totalSize = 0, 0\n",
    "  #  delete_num = 4000\n",
    "   # data_new = smote1(trainLoader)\n",
    "    new_total =5000\n",
    "    for batch, (data, label) in enumerate(trainLoader):\n",
    "        totalBatch += 1\n",
    "        # convert\n",
    "        for i in range(label.shape[0]):\n",
    "            if(label[i]==1):\n",
    "                #global new_total\n",
    "                data[i]=torch.FloatTensor(data_new[new_total])\n",
    "                new_total = new_total+1\n",
    "            \n",
    "    \n",
    "        data, label = data.to(device), label.to(device)\n",
    "        label = label.to(torch.int64)\n",
    "        # calculate\n",
    "        pred = model(data)\n",
    "        loss = lossFunction(pred, label)\n",
    "        # optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # stats\n",
    "        tloss += loss.item()\n",
    "        totalSize += label.shape[0]\n",
    "        correct += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
    "        # print\n",
    "        if (batch+1) % 10 == 0:\n",
    "            nowTime = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "            print(f'[{nowTime}]    Batch: {batch+1:>4}, Loss:{loss.item():>7.6f}, AvgAcc:{100*correct/totalSize:>6.4f}%')\n",
    "    return (tloss/totalBatch, correct/totalSize)\n",
    "\n",
    "\n",
    "def test(testLoader, model, lossFunction, device):\n",
    "    model.eval()\n",
    "    tloss, totalBatch = 0, 0\n",
    "    correct = 0\n",
    "    totalCount = len(testLoader)\n",
    "    with torch.no_grad():\n",
    "        for batch, (data, label) in enumerate(testLoader):\n",
    "            totalBatch += 1\n",
    "            # convert\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            label = label.to(torch.int64)\n",
    "            # calculate\n",
    "            pred = model(data)\n",
    "            loss = lossFunction(pred, label)\n",
    "            # add\n",
    "            tloss += loss.item()\n",
    "            correct += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
    "    return (tloss/totalBatch, correct/totalCount)\n",
    "\n",
    "def del_tensor_ele(arr,index):\n",
    "    arr1 = arr[0:index]\n",
    "    arr2 = arr[index+1:]\n",
    "    return torch.cat((arr1,arr2),dim=0)\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f'Using {device} device')\n",
    "\n",
    "    trainLoader, testLoader = loadData()\n",
    "\n",
    "    model = ResNet56.resnet56().to(device)\n",
    "    lossFunction = nn.CrossEntropyLoss()\n",
    "    lr = 1e-1\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9, weight_decay=1e-4)\n",
    "    data_new = smote1(trainLoader)#选择填补数据的方法\n",
    "    #data_new = smote(trainLoader)\n",
    "    #data_new = smote2(trainLoader)\n",
    "    print(\"Begin Training\")\n",
    "    for epoch in range(64):\n",
    "        if epoch == 32:\n",
    "            lr/=10\n",
    "        elif epoch == 48:\n",
    "            lr/=10\n",
    "\n",
    "        print(f\"Epoch:{epoch+1:>3}, learning rate = {lr}\")\n",
    "        trainLoss, trainAcc = train(trainLoader, model, lossFunction, optimizer, device,data_new)\n",
    "        testLoss, testAcc = test(testLoader, model, lossFunction, device)\n",
    "        with open('./result.txt', 'a') as f:\n",
    "            nowTime = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "            f.write(f'[{nowTime}] Epoch{epoch+1:>3d}\\n')\n",
    "            f.write(f'    Train: Loss {trainLoss:>7.6f}, Acc {trainAcc:>6.4f}%\\n')\n",
    "            f.write(f'    Test:  Loss {testLoss :>7.6f}, Acc {testAcc :>6.4f}%\\n')\n",
    "        torch.save(model.state_dict(), './saves/model.pth')\n",
    "    print(\"Done\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "#batch = 390\n",
    "#print(batch *128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote(trainLoader):\n",
    "    min_data =[]\n",
    "    max_data = []\n",
    "    num =0\n",
    "    delete_num = 4000\n",
    "    for batch, (data, label) in enumerate(trainLoader):\n",
    "        #label_1 = label\n",
    "        ex =0\n",
    "        if (delete_num != 0):\n",
    "            #print(delete_num)\n",
    "            for i in range(label.shape[0]):\n",
    "                if (delete_num == 0):\n",
    "                    break\n",
    "                i = i-ex\n",
    "                if(label[i]==1):\n",
    "                    label = del_tensor_ele(label,i)\n",
    "                    data  = del_tensor_ele(data,i)\n",
    "                    ex = ex+1\n",
    "                    num =num +1\n",
    "                    delete_num = delete_num-1\n",
    "        else:\n",
    "        #print(1)\n",
    "            for i in range(label.shape[0]):\n",
    "                if(label[i]==1):\n",
    "                    min_data.append(data[i])\n",
    "        for i in range(label.shape[0]):\n",
    "            if(label[i]==0):\n",
    "                max_data.append(data[i])\n",
    "\n",
    "                \n",
    "    for i in range(len(min_data)):\n",
    "        min_data[i] = min_data[i].numpy()\n",
    "    min_data = np.array(min_data) \n",
    "#print(\"min 0k\")\n",
    "    for i in range(len(max_data)):\n",
    "        max_data[i] = max_data[i].numpy()\n",
    "    max_data = np.array(max_data)    \n",
    "\n",
    "##转换\n",
    "\n",
    "    shuju =np.vstack((max_data,min_data))\n",
    "    mid = shuju.reshape(5991,-1)\n",
    "    biaoqian = np.ones(5991)\n",
    "    for i in range(len(biaoqian)):\n",
    "        if (i<5000):\n",
    "            biaoqian[i]=0\n",
    "    #print(biaoqian.shape)\n",
    "\n",
    "#smote\n",
    "    \n",
    "#定义SMOTE模型，random_state相当于随机数种子的作用\n",
    "    smo = SMOTE(random_state=42)\n",
    "    X_smo, y_smo = smo.fit_resample(mid, biaoqian)\n",
    "\n",
    "    new_data = X_smo.reshape(10000,3,32,32)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote1(trainLoader):\n",
    "    min_data =[]\n",
    "    max_data = []\n",
    "    num =0\n",
    "    delete_num = 4000\n",
    "    for batch, (data, label) in enumerate(trainLoader):\n",
    "        #label_1 = label\n",
    "        ex =0\n",
    "        if (delete_num != 0):\n",
    "            #print(delete_num)\n",
    "            for i in range(label.shape[0]):\n",
    "                if (delete_num == 0):\n",
    "                    break\n",
    "                i = i-ex\n",
    "                if(label[i]==1):\n",
    "                    label = del_tensor_ele(label,i)\n",
    "                    data  = del_tensor_ele(data,i)\n",
    "                    ex = ex+1\n",
    "                    num =num +1\n",
    "                    delete_num = delete_num-1\n",
    "        else:\n",
    "        #print(1)\n",
    "            for i in range(label.shape[0]):\n",
    "                if(label[i]==1):\n",
    "                    min_data.append(data[i])\n",
    "        for i in range(label.shape[0]):\n",
    "            if(label[i]==0):\n",
    "                max_data.append(data[i])\n",
    "\n",
    "                \n",
    "    for i in range(len(min_data)):\n",
    "        min_data[i] = min_data[i].numpy()\n",
    "    min_data = np.array(min_data) \n",
    "#print(\"min 0k\")\n",
    "    for i in range(len(max_data)):\n",
    "        max_data[i] = max_data[i].numpy()\n",
    "    max_data = np.array(max_data)    \n",
    "\n",
    "##转换\n",
    "\n",
    "    shuju =np.vstack((max_data,min_data))\n",
    "    mid = shuju.reshape(5991,-1)\n",
    "    biaoqian = np.ones(5991)\n",
    "    for i in range(len(biaoqian)):\n",
    "        if (i<5000):\n",
    "            biaoqian[i]=0\n",
    "    #print(biaoqian.shape)\n",
    "\n",
    "#smote\n",
    "    \n",
    "#定义SMOTE模型，random_state相当于随机数种子的作用\n",
    "    smo =BorderlineSMOTE(random_state=42,kind=\"borderline-1\")\n",
    "    X_smo, y_smo = smo.fit_resample(mid, biaoqian)\n",
    "\n",
    "    new_data = X_smo.reshape(10000,3,32,32)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote2(trainLoader):\n",
    "    min_data =[]\n",
    "    max_data = []\n",
    "    num =0\n",
    "    delete_num = 4000\n",
    "    for batch, (data, label) in enumerate(trainLoader):\n",
    "        #label_1 = label\n",
    "        ex =0\n",
    "        if (delete_num != 0):\n",
    "            #print(delete_num)\n",
    "            for i in range(label.shape[0]):\n",
    "                if (delete_num == 0):\n",
    "                    break\n",
    "                i = i-ex\n",
    "                if(label[i]==1):\n",
    "                    label = del_tensor_ele(label,i)\n",
    "                    data  = del_tensor_ele(data,i)\n",
    "                    ex = ex+1\n",
    "                    num =num +1\n",
    "                    delete_num = delete_num-1\n",
    "        else:\n",
    "        #print(1)\n",
    "            for i in range(label.shape[0]):\n",
    "                if(label[i]==1):\n",
    "                    min_data.append(data[i])\n",
    "        for i in range(label.shape[0]):\n",
    "            if(label[i]==0):\n",
    "                max_data.append(data[i])\n",
    "\n",
    "                \n",
    "    for i in range(len(min_data)):\n",
    "        min_data[i] = min_data[i].numpy()\n",
    "    min_data = np.array(min_data) \n",
    "#print(\"min 0k\")\n",
    "    for i in range(len(max_data)):\n",
    "        max_data[i] = max_data[i].numpy()\n",
    "    max_data = np.array(max_data)    \n",
    "\n",
    "##转换\n",
    "\n",
    "    shuju =np.vstack((max_data,min_data))\n",
    "    mid = shuju.reshape(5991,-1)\n",
    "    biaoqian = np.ones(5991)\n",
    "    for i in range(len(biaoqian)):\n",
    "        if (i<5000):\n",
    "            biaoqian[i]=0\n",
    "    #print(biaoqian.shape)\n",
    "\n",
    "#smote\n",
    "    \n",
    "#定义SMOTE模型，random_state相当于随机数种子的作用\n",
    "    smo =BorderlineSMOTE(random_state=42,kind=\"borderline-2\")\n",
    "    X_smo, y_smo = smo.fit_resample(mid, biaoqian)\n",
    "\n",
    "    new_data = X_smo.reshape(10000,3,32,32)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbors NearestNeighbors()\n",
      "[[1.12140022 2.12140022 2.75719956]\n",
      " [2.34428963 3.34428963 4.34428963]\n",
      " [1.98260712 2.98260712 1.03478575]\n",
      " [2.         2.03516405 3.03516405]\n",
      " [2.         3.         4.        ]\n",
      " [2.         3.         4.        ]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "class Smote:\n",
    "    def __init__(self,samples,N=10,k=5):\n",
    "        self.n_samples,self.n_attrs=samples.shape\n",
    "        self.N=N\n",
    "        self.k=k\n",
    "        self.samples=samples\n",
    "        self.newindex=0\n",
    "       # self.synthetic=np.zeros((self.n_samples*N,self.n_attrs))\n",
    "\n",
    "    def over_sampling(self):\n",
    "        N=int(self.N/100)\n",
    "        self.synthetic = np.zeros((self.n_samples * N, self.n_attrs))\n",
    "        neighbors=NearestNeighbors(n_neighbors=self.k).fit(self.samples)\n",
    "        print ('neighbors',neighbors)\n",
    "        for i in range(len(self.samples)):\n",
    "            nnarray=neighbors.kneighbors(self.samples[i].reshape(1,-1),return_distance=False)[0]\n",
    "            #print nnarray\n",
    "            self._populate(N,i,nnarray)\n",
    "        return self.synthetic\n",
    "\n",
    "\n",
    "    # for each minority class samples,choose N of the k nearest neighbors and generate N synthetic samples.\n",
    "    def _populate(self,N,i,nnarray):\n",
    "        for j in range(N):\n",
    "            nn=random.randint(0,self.k-1)\n",
    "            dif=self.samples[nnarray[nn]]-self.samples[i]\n",
    "            gap=random.random()\n",
    "            self.synthetic[self.newindex]=self.samples[i]+gap*dif\n",
    "            self.newindex+=1\n",
    "a=np.array([[1,2,3],[4,5,6],[2,3,1],[2,1,2],[2,3,4],[2,3,4]])\n",
    "s=Smote(a,N=100)\n",
    "print( s.over_sampling())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
